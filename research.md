---
layout: page
title: Research
permalink: /research/
---

{% include image.html url="Arrykrishna_Bayesian.jpg" caption="Myself at the InScida workshop organised at AIMS, Muizenburg" width=400 align="left" %}

<p align="justify">My research is mostly based on Bayesian Statistics. Recently, I have been looking at alternative ways of doing Bayesian Model Comparison. In literature, thermodynamic integration, nested sampling and reversible jump Monte Carlo are other methods of doing model comparison. In addition, I am working on the application of Bayesian Methods in Radio Astronomy. Recently, we have looked into the problem of not only determining the type of source but also to predict the distance between two point sources, which are on top of each other but buried in noise.</p>

<p align="justify">Moreover, I am also interested in Machine Learning and Deep Learning. I like to understand the concepts first and hence, I also work a little bit on the Mathematics of Machine Learning before proceeding to the Computing part. </p>


# Publications

Below is an updated list of my publications.

<ol type="1">
  <li><b>Bayes Factors via Savage-Dickey Supermodels</b> (<a href="https://arxiv.org/abs/1609.02186">arXiv:1609.02186</a>)</li>
  <p align="justify">We outline a new method to compute the Bayes Factor for model selection which bypasses the Bayesian Evidence. Our method combines multiple models into a single, nested, Supermodel using one or more hyperparameters. Since the models are now nested the Bayes Factors between the models can be efficiently computed using the Savage-Dickey Density Ratio (SDDR). In this way model selection becomes a problem of parameter estimation. We consider two ways of constructing the supermodel in detail: one based on combined models, and a second based on combined likelihoods. We report on these two approaches for a Gaussian linear model for which the Bayesian evidence can be calculated analytically and a toy nonlinear problem. Unlike the combined model approach, where a standard Monte Carlo Markov Chain (MCMC) struggles, the combined-likelihood approach fares much better in providing a reliable estimate of the log-Bayes Factor. This scheme potentially opens the way to computationally efficient ways to compute Bayes Factors in high dimensions that exploit the good scaling properties of MCMC, as compared to methods such as nested sampling that fail for high dimensions.</p>

</ol>
