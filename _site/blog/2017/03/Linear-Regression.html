<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Linear Regression</title>
  <meta name="description" content="One common problem in Statistics is to learn a functional relationship between independent variables and dependent variable. For example, we may want to know...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://Harry45.github.io/blog/2017/03/Linear-Regression">
  <link rel="alternate" type="application/rss+xml" title="Arrykrishna Mootoovaloo" href="https://Harry45.github.io/feed.xml" />
<link rel='stylesheet' id='open-sans-css'  href='//fonts.googleapis.com/css?family=Open+Sans%3A300italic%2C400italic%2C600italic%2C300%2C400%2C600&#038;subset=latin%2Clatin-ext&#038;ver=4.2.4' type='text/css' media='all' />
<link href='//fonts.googleapis.com/css?family=Titillium+Web:600italic,600,400,400italic' rel='stylesheet' type='text/css'>




</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Arrykrishna Mootoovaloo</a>


    <nav class="site-nav">

      <a href="#" class="menu-icon menu.open">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>  

    <div class="trigger"><h1>Main Navigation</h1>

 <ul class="menu">

    
    
     <li><a href="/about/" class="page-link">About</a>
    
    </li>
    
    
     <li><a href="/blog/" class="page-link">Blog</a>
    
    </li>
    
    
    <li><a href="/teaching/" class="page-link">Teaching</a>
    <ul class="sub-menu">
    
    <li><a href="/teaching/A-Level-Mathematics/">A-Level Mathematics (9709)</a></li>
    
    <li><a href="/teaching/A-Level-Further-Mathematics/">A-Level Further Mathematics (9231)</a></li>
    
    <li><a href="/teaching/A-Level-Physics/">A-Level Physics (9702)</a></li>
    
    </ul>
    
    </li>
    
    
     <li><a href="/research/" class="page-link">Research</a>
    
    </li>
    
    
    <li><a href="/travel/" class="page-link">Travel</a>
    <ul class="sub-menu">
    
    <li><a href="/travel/Africa/">Africa</a></li>
    
    <li><a href="/travel/Europe/">Europe</a></li>
    
    </ul>
    
    </li>
    
    </ul>


<!-- <ul class="menu">
        <li> <a class="page-link" href="/about">About</a></li>
        <li> <a class="page-link"  href="/blog">Blog</a>
        <li> <a class="page-link" href="/blog">CV</a>
        <li> <a class="page-link" href="/blog">For Students</a></li>
        <li> <a class="page-link"  href="/blog">Research</a></a>
        <li> <a class="page-link" href="/blog">Teaching</a>
<ul class="sub-menu">
	<li><a href="http://svmiller.com/teaching/posc-1020-introduction-to-international-relations/">POSC 1020 – Introduction to International Relations</a></li>
	<li><a href="http://svmiller.com/teaching/posc-3410-quantitative-methods-in-political-science/">POSC 3410 – Quantitative Methods in Political Science</a></li>
	<li><a href="http://svmiller.com/teaching/posc-3610-international-politics-in-crisis/">POSC 3610 – International Politics in Crisis</a></li>
	<li><a href="http://svmiller.com/teaching/posc-3630-united-states-foreign-policy/">POSC 3630 – United States Foreign Policy</a></li>
</ul></li>
        <li> <a class="page-link" href="/blog">Miscellany</a>
<ul class="sub-menu">
	<li><a href="http://svmiller.com/teaching/posc-1020-introduction-to-international-relations/">Clean USAID Greenbook Data</a></li>
	<li><a href="http://svmiller.com/teaching/posc-3410-quantitative-methods-in-political-science/">Journal of Peace Research *.bst File</a></li>
	<li><a href="http://svmiller.com/teaching/posc-3610-international-politics-in-crisis/">My Custom Beamer Style</a></li>
</ul> 

</li>
</ul> -->

     </div>  
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Linear Regression</h1>
    <p class="post-meta">Posted on March 2, 2017 by  A.Mootoovaloo  

  in
  
  <a href="/categories/#Machine Learning and Statistics" title="Machine Learning and Statistics">Machine Learning and Statistics</a>&nbsp;
  


</p>
  </header>

  <article class="post-content">
    <p align="justify">One common problem in Statistics is to learn a functional relationship between independent variables and dependent variable. For example, we may want to know how the price of houses varies with the area of the land, the total size of the house and various other criteria. In this particular case, the price of the houses is the dependent variable, also often referred to as the response variable while the area of the land and total size of the house are the independent variables, also known as the attribute variables.</p>

<p align="justify">We will first begin with linear modelling, that is, given a set of attributes, we want infer a linear relationship between the attributes and the response. The function which we want to fit is typically governed by a set of parameters, say $\theta_{i}$. However, before indulging into this topic, it is worth discussing linear and non-linear models.</p>

<div style="background-color: #FFF8C6; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px;">
<b>Linear and Non-Linear Models</b><br />
The equation

\begin{align}
y=\theta_{0} + \theta_{1}x + \theta_{2}x^2
\end{align}

is a linear model model because it is linear in the parameters $\theta_{i}$. In contrast, the model 

\begin{align}
y=\textrm{sin}\left(\omega x + \phi\right)
\end{align}

is a non-linear model since it includes parameters $\left(\omega,\,\phi\right)$ which are non-linear. 
 
</div>

<h2>Maximum Likelihood Method</h2>
<p align="justify">Suppose, we now want to fit a polynomial $f\left(x\right)$ of order $M$ to some observed data $\left(x_{i},\,y_{i}\right)$ where $i=0,\,1,\,2,\ldots N-1$. We further assume that they are corrupted with Gaussian noise, $\sigma_{i}$. Then, each observed datum can be described by a Gaussian:

\begin{align}
\mathcal{P}\left(y_{i}\left|\boldsymbol{\theta}\right.\right)=\dfrac{1}{\sqrt{2\pi\sigma_{i}^{2}}}\,\textrm{exp}\left[-\dfrac{1}{2}\left(\dfrac{y_{i}-f\left(x_{i}\left|\boldsymbol{\theta}\right.\right)}{\sigma_{i}}\right)^{2}\right]
\end{align}

where $\boldsymbol{\theta}$ is the set of parameters $\left(\theta_{0},\,\theta_{1},\ldots \theta_{M}\right)$. For simplicity, we will explain this method by using a linear fit to the data, that is, $f\left(x\left|\theta_{0},\,\theta_{1}\right.\right)=\theta_{0}+\theta_{1}x$. Assuming that the data point is independent from each other, the likelihood is simply a product of the individual probability distribution of the above. 

</p>

<div style="background-color: #FFF8C6; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px;">
<b>Matrix and Vector Notations</b><br />
We will define the vector $\mathbf{b}$ as:

$$
\mathbf{b=\left(\begin{array}{c}
\frac{y_{0}}{\sigma_{0}}\\
\\
\vdots\\
\\
\frac{y_{N-1}}{\sigma_{N-1}}
\end{array}\right)}
$$

and the design matrix, $\mathbf{D}$ as: 

$$
\mathbf{D}=\left(\begin{array}{cc}
\frac{1}{\sigma_{0}} &amp; \frac{x_{0}}{\sigma_{0}}\\
\\
\\
\\
\frac{1}{\sigma_{N-1}} &amp; \frac{x_{N-1}}{\sigma_{N-1}}
\end{array}\right)
 
$$
</div>

<p align="justify">Therefore, the likelihood (ignoring the pre-factor) can be written as:

\begin{align}
\mathcal{P}\left(\mathbf{y}\left|\boldsymbol{\theta}\right.\right)\propto\textrm{exp}\left[-\dfrac{1}{2}\left(\mathbf{b-\mathbf{D}\boldsymbol{\theta}}\right)^{\textrm{T}}\left(\mathbf{b-\mathbf{D}\boldsymbol{\theta}}\right)\right]
\label{eq:likelihood}
\end{align}

Our aim is to maximise the likelihood. Therefore, the derivative of the likelihood with respect to the parameters should be equal to zero. However, maximising the likelihood is equivalent to maximising the log-likelihood since the latter is a monotonic transformation applied to the likelihood. Some useful tricks when differentiating with respect to a vector are given below.</p>

<div style="background-color: #FFF8C6; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px;">
<b>Differentiating with respect to a vector</b><br />

$$
\dfrac{\partial\mathbf{A}\mathbf{x}}{\partial\mathbf{x}}=\mathbf{A}
$$

$$
\dfrac{\partial\mathbf{x}^{\textrm{T}}\mathbf{A}}{\partial\mathbf{x}}=\mathbf{A}^{\textrm{T}}
$$

If $\mathbf{A}$ is a symmetric matrix, 
$$
\dfrac{\partial\mathbf{x}^{\textrm{T}}\mathbf{A}\mathbf{x}}{\partial\mathbf{x}}=2\mathbf{x}^{\textrm{T}}\mathbf{A}
$$

For further details, see <a href="https://en.wikipedia.org/wiki/Matrix_calculus">Wikipedia</a> or the <a href="http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf">Matrix Cookbook.</a>
</div>

<p align="justify">Using Equation \eqref{eq:likelihood} and the above useful identities, </p>

<p>\begin{align}
\boldsymbol{\theta}_{\textrm{MLE}}=\left(\mathbf{D}^{\textrm{T}}\mathbf{D}\right)^{-1}\mathbf{D}^{\textrm{T}}\mathbf{b}
\end{align}</p>

<p align="justify"> The matrix $\mathbf{C}=\left(\mathbf{D}^{\textrm{T}}\mathbf{D}\right)^{-1}$ is called the covariance matrix and gives the standard uncertainties associated with the parameters determined. In particular, the diagonal elements of this matrix give the variances of the parameters while the off-diagonal elements give the covariances between the parameters $\theta_{j}$ and $\theta_{k}$. Hence, the errors on the parameters are equal to the square root of the diagonal elements of the covariance matrix $\mathbf{C}$.</p>

<h2>Example - A Physics Problem</h2>

<p><img src="/images/Linear_Regression_Circuit.png" align="left" width="420" /></p>

<p align="justify">We will use the above explanation to answer a Physics problem (Physics 9702 November 2016 Paper 52). A student is investigating the characteristics of different light-emitting diodes (LEDs). Each LED
needs a minimum potential difference across it to emit light. The circuit is set up as shown on the left. </p>

<p align="justify">The potentiometer is adjusted until the LED just emits light. The potential difference $V$ across the LED is measured. The experiment is repeated for LEDs that emit light of different wavelength $\lambda$. It is suggested that $V$ and $\lambda$ are related by the equation

\begin{align}
V=p\lambda^{q}
\end{align}

where $p$ and $q$ are constants. Therefore, if we are to plot a graph of $\textrm{lg }V$ on the $y$-axis against $\textrm{lg }\lambda$ on the $x$-axis, the gradient of the straight line will correspond to $q$ while the $y$-intercept to $\textrm{lg }p$. To be explicit, 

$$
\textrm{lg }V = q\,\textrm{lg }\lambda + \textrm{lg }p
$$
</p>

<p><img src="/images/Linear_Regression_Data.png" align="right" width="420" /></p>

<p align="justify">The values of $V$ and $\lambda$ are given in the table below and we also calculate $\textrm{lg }\lambda$ and $\textrm{lg }V$, with its associated error. The error in $\textrm{lg }V$ is:

$$
\sigma_{\textrm{lg }V} = \dfrac{\Delta V}{V}
$$

See this <a href="http://phys114115lab.capuphysics.ca/App%20A%20-%20uncertainties/appA%20propLogs.htm">link</a> for further detail. For this problem, we calculate $\textrm{lg }\lambda$ and $\textrm{lg }V$ to two decimal places. We assume that each data point is Gaussian distributed with mean, $\mu=\textrm{lg }V$ and standard deviation, $\sigma = \sigma_{\textrm{lg }V}$. An illustration of this assumption is shown on the right. We are now ready to plot the graph of $\textrm{lg }V$ versus $\textrm{lg }\lambda$. We find that the gradient is equal to $2.60$ while the $y$-intercept is equal to $7.56$.</p>

<style>
table, td, th {
    border: 1px solid black;
}

table {
    border-collapse: collapse;
    width: 53%;
}

</style>

<style type="text/css">
	table.tableizer-table {
		font-size: 13px;
		border: 1px solid #CCC; 
		font-family: Arial, Helvetica, sans-serif;
	} 
	.tableizer-table td {
		padding: 6px;
		margin: 3px;
		border: 1px solid #CCC;
	}
	.tableizer-table th {
		background-color: #104E8B;
		padding: 6px; 
		color: #FFF;
		font-weight: bold;
	}
</style>

<table class="tableizer-table" align="left">
<thead><tr class="tableizer-firstrow"><th>$\lambda/10^{-9}$ m </th><th>$V/\,\textrm{V}$</th><th>$\textrm{lg}\left(\lambda/10^{-9}\,\textrm{m}\right)$</th><th>$\textrm{lg}\left(V/\,\textrm{V}\right)$</th></tr></thead><tbody>
 <tr><td align="center">630</td><td align="center">$1.9\pm0.1$</td><td align="center">2.80</td><td align="center">$ 0.28\pm0.05$ </td></tr>
 <tr><td align="center">620</td><td align="center">$2.0\pm0.1$</td><td align="center">2.79</td><td align="center">$0.30\pm0.05$ </td></tr>
 <tr><td align="center">590</td><td align="center">$2.3\pm0.1$</td><td align="center">2.77</td><td align="center">$0.36\pm0.04$</td></tr>
 <tr><td align="center">520</td><td align="center">$3.1\pm0.1$</td><td align="center">2.72</td><td align="center">$0.49\pm0.03$</td></tr>
 <tr><td align="center">490</td><td align="center">$3.7\pm0.1$</td><td align="center">2.69</td><td align="center">$0.57\pm0.03$</td></tr>
 <tr><td align="center">470</td><td align="center">$4.1\pm0.1$</td><td align="center">2.67</td><td align="center">$0.61\pm0.02$</td></tr>
</tbody></table>

<p align="justify" style="margin-left:32em">Moreover, the covariance matrix is: 

$$
\mathbf{C}=\left(\begin{array}{cc}
0.670 &amp; -0.258\\
-0.258 &amp; 0.095
\end{array}\right)
 
$$

Hence, 

$$
q=-2.60\pm0.84
$$

$$
\textrm{lg }p = 7.56\pm0.31
$$
</p>
<p><img src="/images/Linear_Regression_Fit.png" align="right" width="420" /></p>

<p align="justify" style="margin-top:3em">Therefore, in short, the estimates of $p$ and $q$ are $3.61\times10^{7}$ and $-2.60$ respectively. We can also learn about the correlation between the two parameters from the covariance matrix. In particular, since the off-diagonal elements are negative, this implies that the two parameters are negatively correlated, that is, an increase in one parameter results in the decrease of the other. This is illustrated in the figure below, which also shows the credible intervals at $1\sigma$, $2\sigma$ and $3\sigma$. It is also worth mentioning that the distribution of the two parameters $\left(\textrm{lg }p,\,q\right)$ are Gaussian distributed since we are working with a linear model. In addition to this, we can use the values of $p$ and $q$ to estimate the minimum potential difference required if a different diode is used, for example, a diode emitting a wavelength of $950$ nm. 
</p>

<p><img src="/images/Linear_Regression_Correlation.png" align="left" width="420" /></p>

<h2>Summary and Conclusion</h2>
<p align="justify" style="margin-left:28em">In this post, we have gone through one method of inferring parameters, namely, the Maximum Likelihood Estimator. We have also provided an example to illustrate this method. It does provide a good estimate of the parameters along with their associated errors. Moreover, we can also learn about the correlation between the parameters using the covariance matrix. </p>

<p align="justify" style="margin-left:28em">In addition to this, if we had some prior information on the parameters, then the concept of Bayesian Statistics is invoked. The concept is not very different, except that we would have had a prior probability distribution for each parameter. Additionally, instead of finding the MLE, we will end up having the MAP, that is, the Maximum a Posteriori estimates of the parameters. If uniform distributions are assumed, then the MAP coincide with the MLE.</p>


  </article>

  <!-- mathjax -->
  
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });

  </script>
  <script type="text/javascript"
        src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  

</div>


      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

<!--     <h2 class="footer-heading">Arrykrishna Mootoovaloo</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li><strong>Arrykrishna Mootoovaloo</strong></li>

          <li><a href="mailto:arrykrish@gmail.com">arrykrish@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/Harry45">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">Harry45</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/arrykrishna">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">arrykrishna</span>
            </a>
          </li>
          


          
          <li>
            <a href="https://facebook.com/Arrykrishna">
              <span class="icon  icon--facebook">
                <svg viewBox="0 0 90 90">
                  <path fill="#828282" d="M90,15.001C90,7.119,82.884,0,75,0H15C7.116,0,0,7.119,0,15.001v59.998
    C0,82.881,7.116,90,15.001,90H45V56H34V41h11v-5.844C45,25.077,52.568,16,61.875,16H74v15H61.875C60.548,31,59,32.611,59,35.024V41
    h15v15H59v34h16c7.884,0,15-7.119,15-15.001V15.001z"/>
                </svg>
              </span>

              <span class="username">Arrykrishna</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://linkedin.com/in/arrykrishna">
              <span class="icon  icon--linkedin">
                <svg viewBox="0 0 506 506">
                  <path fill="#828282" d="M398.355,0H31.782C14.229,0,0.002,13.793,0.002,30.817v368.471
    c0,17.025,14.232,30.83,31.78,30.83h366.573c17.549,0,31.76-13.814,31.76-30.83V30.817C430.115,13.798,415.904,0,398.355,0z
     M130.4,360.038H65.413V165.845H130.4V360.038z M97.913,139.315h-0.437c-21.793,0-35.92-14.904-35.92-33.563
    c0-19.035,14.542-33.535,36.767-33.535c22.227,0,35.899,14.496,36.331,33.535C134.654,124.415,120.555,139.315,97.913,139.315z
     M364.659,360.038h-64.966V256.138c0-26.107-9.413-43.921-32.907-43.921c-17.973,0-28.642,12.018-33.327,23.621
    c-1.736,4.144-2.166,9.94-2.166,15.728v108.468h-64.954c0,0,0.85-175.979,0-194.192h64.964v27.531
    c8.624-13.229,24.035-32.1,58.534-32.1c42.76,0,74.822,27.739,74.822,87.414V360.038z M230.883,193.99
    c0.111-0.182,0.266-0.401,0.42-0.614v0.614H230.883z"/>
                </svg>
              </span>

              <span class="username">arrykrishna</span>
            </a>
          </li>
          




        </ul>
      </div>

      <div class="footer-col  footer-col-3">
         <p class="text">
22, Derwentwater Road, Acton Town, London (W3 6DE) 
      </div>
    </div>

  </div>

</footer>




  </body>

</html>
