<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arrykrishna Mootoovaloo</title>
    <description>22, Derwentwater Road, Acton Town, London (W3 6DE)</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 25 Nov 2021 14:12:23 +0000</pubDate>
    <lastBuildDate>Thu, 25 Nov 2021 14:12:23 +0000</lastBuildDate>
    <generator>Jekyll v4.1.1</generator>
    
      <item>
        <title>Emulator for the 3D Matter Power Spectrum</title>
        <description>&lt;p align=&quot;justify&quot;&gt;The 3D matter power spectrum, $P_{\delta}(k,z)$ is a key quantity which underpins most cosmological data analysis, such as galaxy clustering, weak lensing, 21 cm cosmology and various others. Crucially, the calculation of other (derived) power spectra can be fast if $P_{\delta}(k,z)$ is precomputed. In practice, the latter is the most expensive component and can be calculated either using Boltzmann solvers such as CLASS or CAMB, or via simulations, which can be computationally expensive depending on the resolution of the experiments. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/3D_pk.jpg&quot; align=&quot;left&quot; width=&quot;400&quot; style=&quot;margin-right: 10px; margin-bottom: 10px&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;This work was published in the &lt;a href=&quot;https://doi.org/10.1016/j.ascom.2021.100508&quot;&gt;Astronomy and Computing&lt;/a&gt; journal. Our contributions in this work are three fold. First, it addresses the point that we do not always need to assume a zero mean Gaussian Process model for performing emulation, in other words, one can also include some additional basis functions prior to defining the kernel matrix. This can be useful if we already have an approximate model of our function. Moreover, if we know how a particular function behaves, one can adopt a stringent prior on the regression coefficients for the parametric model, hence allowing us to encode our degree of belief about that specific parametric model. Second, since we are using a Radial Basis Function (RBF) kernel and the fact that it is infinitely differentiable enables us to estimate the first and second derivatives of the 3D matter power spectrum. The derived expressions for the derivatives also indicate that there is only element-wise matrix multiplication and no matrix inverse to compute. This makes the gradient calculations very fast. Finally, with the approach that we adopt, we show that the emulator can output various key power spectra, namely, the linear matter power spectrum at a reference redshift $z_{0}$ and the non-linear 3D matter power spectrum with/without an analytic baryon feedback model. Moreover, using the emulated 3D power spectrum and the tomographic redshift distributions, we also show that the weak lensing power spectrum and the intrinsic alignment (II and GI) can be generated in a very fast way using existing numerical techniques. The 3D matter power spectrum can be decomposed as  

$$
P_{\delta}(k,z)=D(z)[1+q(k,z)]P_{\textrm{lin}}(k,z_{0})
$$

Each component $D(z)$, $q(k,z)$ and $P_{\textrm{lin}}(k,z_{0})$ are then modelled as separate semi-parametric Gaussian Processes, where we assume a second order polynomial function for the parametric part.&lt;/p&gt;

&lt;dl class=&quot;wp-caption aligncenter&quot; style=&quot;max-width: 800px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/all_gradients.jpg&quot; alt=&quot;The gradients of the power spectrum with respect to each cosmological parameter at a fixed redshift.&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;The gradients of the power spectrum with respect to each cosmological parameter at a fixed redshift.&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;In the figure above, we show the gradient along at a fixed cosmological parameter (test point) and fixed redshift, $z=0$. The red curve corresponds to the gradients as calculated by CLASS using central difference method and the blue curves show the gradients output from the emulator. In particular, this gradient is strictly a 3D quantity, as a function of the wavenumber $k$, redshift, $z$ and the cosmological parameters $\boldsymbol{\theta}$. In other words, the gradient calculation from the emulator will be a tensor of size $(N_{k},\,N_{z},\,N_{p})$, where $N_{k}$ is the number of wavenumbers for $k\in[5\times 10^{-4},\,50]$, $N_{z}$ is the number of redshifts for $z\in[0.0,\,5]$ and $N_{p}$ is the number of parameters considered. In this case, $N_{p}=5$ and the default values for a finer grid in $k$ and $z$ are $N_{k}=1000$ and $N_{z}=100$ respectively.&lt;/p&gt;

&lt;dl class=&quot;wp-caption aligncenter&quot; style=&quot;max-width: 600px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/simulator_emulator.jpg&quot; alt=&quot;The full posterior distribution of all parameters using the emulator on a toy dataset.&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;The full posterior distribution of all parameters using the emulator on a toy dataset.&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;We also test the emulator on simulated weak-lensing bandpowers. We assume measurements over $10\leq\ell\leq 1500$ and 5 tomographic slices with Gaussian $n(z)$ centred on redshifts [0.5, 1.0, 1.5, 2.0, 2.5] and each having a standard deviation of 0.075. Ten bandpowers, equally spaced in logarithmic scale, are used and this gives us a set of 150 data points. Moreover, we simulate and then assume in the likelihood  independent Gaussian errors with, for simplicity, $\sigma=0.5\hat{\mathcal{B}}_{\ell}$, where $\hat{\mathcal{B}}_{\ell}$ is the bandpower evaluated at the fiducial set of cosmological parameters. For this particular case, we have set $A_{\textrm{IA}}=0$ but one can trivially include this factor and marginalise over it in the sampling process. The fiducial point $\boldsymbol{\theta}_{\textrm{fid}}=[0.12, 0.0225, 3.45, 1.0, 0.72]$ is used to generate the data and is shown by the black dots in the figure above. We use a Gaussian likelihood and uniform priors on all cosmological parameters, similar to the range of the inputs of the emulator. The figure below shows the results obtained when sampling the cosmological parameters on this toy data set. The red contours correspond to the result using the emulator while the pale blue colour refers to the posterior distributions using CLASS. We run three separate MCMC chains, each with 150 000 MCMC samples, two with the emulator and one with CLASS. On each of the three resulting pairs of runs, we compute the Gelman-Rubin convergence parameter. The worst $\hat{R}$ value is 1.002, consistent with all three chains being drawn from the same distribution, and corroborating the agreement shown in the above figure.  The emulator developed in this work is thus able to robustly recover the posterior distributions of all the cosmological parameters, compared to the accurate solver, CLASS.&lt;/p&gt;

</description>
        <pubDate>Tue, 16 Nov 2021 07:11:00 +0000</pubDate>
        <link>http://localhost:4000/blog/2021/11/Imperial-Publication-2</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2021/11/Imperial-Publication-2</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>Parameter Inference with MOPED and GP</title>
        <description>&lt;p align=&quot;justify&quot;&gt;In this post, I will cover briefly my first paper which was published in &lt;a href=&quot;https://academic.oup.com/mnras/article/497/2/2213/5873022&quot;&gt;MNRAS&lt;/a&gt; during my PhD. An important step in this paper is the compression and emulation of the MOPED coefficients. MOPED is an algorithm developed by &lt;a href=&quot;https://academic.oup.com/mnras/article/317/4/965/1039456&quot;&gt;Heavens et al. 2000&lt;/a&gt;, which essentially compresses a data vector of size $N$ to just $p$ numbers, where $p$ is the number of parameters in the model. The first and subsequent MOPED vectors are given respectively by&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathbf{b}_{1}=\frac{\mathbf{C}^{-1}\mathbf{\mu}_{,1}}{\sqrt{\mathbf{\mu}_{,1}^{\textrm{T}}\mathbf{C}^{-1}\mathbf{\mu}_{,1}}}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;and&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathbf{b}_{\alpha}=\frac{\mathbf{C}^{-1}\mathbf{\mu}_{,\alpha}-\sum_{\beta=1}^{\alpha-1}(\mathbf{\mu}_{,\alpha}^{\textrm{T}}\mathbf{b}_{\beta})\mathbf{b}_{\beta}}{\sqrt{\mathbf{\mu}_{,\alpha}^{\textrm{T}}\mathbf{C}^{-1}\mathbf{\mu}_{,\alpha}-\sum_{\beta=1}^{\alpha-1}(\mathbf{\mu}_{,\alpha}^{\textrm{T}}\mathbf{b}_{\beta})^{2}}}\;\;(\alpha&amp;gt;1).
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt; The weighing vector, $\mathbf{b}$ encapsulates as much information as possible for a specific model parameter $\mathbf{\theta}_{\alpha}$. This vector is then used to find linear combination of the data, $\mathbf{d}$ such that the compressed data is&lt;/p&gt;

&lt;p&gt;\begin{align}
y_{\alpha}\equiv\mathbf{b}^{\textrm{T}}_{\alpha}\mathbf{d}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;and the expected theoretical prediction is simply $y_{\alpha}\equiv\mathbf{b}^{\textrm{T}}_{\alpha}\mathbf{\mu}$. One would then use an MCMC algorithm to sample the posterior distribution of the model parameters using the MOPED compression algorithm. In particular, each MOPED vector is orthogonal to each other and is also normalised, hence the log-likelihood is simply&lt;/p&gt;

&lt;p&gt;\begin{align}
\textrm{log}\,\mathcal{L} = -\frac{1}{2}\sum_{\alpha=1}^{p}(y_{\alpha}-\mathbf{b}_{\alpha}^{\textrm{T}}\mathbf{\mu})^{2}  + \textrm{constant}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;However, calculating the MOPED coefficient at each step in an MCMC can be expensive if the forward model itself is expensive. Hence, a training set is first generated at $N$ Latin Hypercube samples (LHS) and the MOPED coefficients are computed at these points, followed by modelling $p$ separate Gaussian Processes. These are then used as surrogates to sample the posterior distribution of the model parameters and the result is shown in the figure below. The figure shows the posterior with the full accurate solver CLASS (in tan) and the posterior due to the emulator is shown in blue. The contours are plotted at 68% and 95% credible intervals.&lt;/p&gt;

&lt;dl class=&quot;wp-caption aligncenter&quot; style=&quot;max-width: 800px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/triangle_plot_derived_sigma_8_semi_gp_maximin_1000_7D.jpg&quot; alt=&quot;The full posterior distribution of all parameters using the MOPED compression scheme.&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;The full posterior distribution of all parameters using the MOPED compression scheme.&lt;/dd&gt;
&lt;/dl&gt;

</description>
        <pubDate>Tue, 16 Nov 2021 07:11:00 +0000</pubDate>
        <link>http://localhost:4000/blog/2021/11/Imperial-Publication-1</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2021/11/Imperial-Publication-1</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>Machine Learning Resources</title>
        <description>&lt;p align=&quot;justify&quot;&gt;Below is a curated list of courses/lectures/schools which I often use as references for learning Machine Learning techniques. Under Deep Learning, there are various other branches such as Natural Language Processing, Meta Learning and various others. For the latter, we will separately list the resouces available online.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;General Machine Learning&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;

&lt;li&gt; The AI Epiphany (&lt;a href=&quot;https://www.youtube.com/channel/UCj8shE7aIn4Yawwbo2FceCQ&quot;&gt;Videos&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt; ML Tech Talks (&lt;a href=&quot;https://www.youtube.com/playlist?list=PLQY2H8rRoyvwmjfn7hM-Yg_6RIyoMnKQx&quot;&gt;Videos&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;CPSC 540: Machine Learning 2013 (&lt;a href=&quot;https://www.cs.ubc.ca/~nando/540-2013/lectures.html&quot;&gt;Lecture Notes&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6&quot;&gt;Videos&lt;/a&gt;) by Prof. Nando de Freitas&lt;/li&gt;

&lt;li&gt;Machine Learning Summer School 2013 (&lt;a href=&quot;http://mlss.tuebingen.mpg.de/2013/2013/speakers.html&quot;&gt;Lecture Notes&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/playlist?list=PLqJm7Rc5-EXFv6RXaPZzzlzo93Hl0v91E&quot;&gt;Videos&lt;/a&gt;)&lt;/li&gt;

&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;Deep Learning&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;

&lt;li&gt;Deep Learning with PyTorch (&lt;a href=&quot;https://atcold.github.io/pytorch-Deep-Learning/&quot;&gt;Lecture Notes&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&quot;&gt;Videos&lt;/a&gt;) by Prof. Yann LeCun and Alfredo Canziani&lt;/li&gt;

&lt;li&gt;MIT 6. S191 Introduction to Deep Learning (&lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;Lecture Notes&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&quot;&gt;Videos&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;Deep Learning (&lt;a href=&quot;https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/&quot;&gt;Lecture Notes&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu&quot;&gt;Videos&lt;/a&gt;) by Prof. Nando de Freitas&lt;/li&gt;

&lt;li&gt;DeepMind x UCL 2020 (&lt;a href=&quot;https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&quot;&gt;Lectures and Videos&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;EE-559 – Deep Learning 2019 (&lt;a href=&quot;https://fleuret.org/ee559/&quot;&gt;Lecture Notes and Videos&lt;/a&gt;) by Prof. Fran&amp;ccedil;ois Fleuret&lt;/li&gt;

&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;Reinforcement Learning&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;

&lt;li&gt; 2021 DeepMind x UCL Reinforcement Learning Lecture Series (&lt;a href=&quot;https://www.youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&quot;&gt;Videos&lt;/a&gt;, Slides below YouTube Video)&lt;/li&gt;

&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;Meta Learning&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;

&lt;li&gt;CS 330: Deep Multi-Task and Meta Learning 2020 (&lt;a href=&quot;https://cs330.stanford.edu/&quot;&gt;Lectures&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&quot;&gt;Videos&lt;/a&gt;) by Prof. Chelsea Finn&lt;/li&gt;

&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;Causal Inference&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;

&lt;li&gt;Introduction to Causal Inference 2020 (&lt;a href=&quot;https://www.bradyneal.com/causal-inference-course&quot;&gt;Lectures and Videos&lt;/a&gt;) by Brady Neal&lt;/li&gt;

&lt;li&gt;Causal inference meets probabilistic models (&lt;a href=&quot;https://www.youtube.com/playlist?list=PLZ_xn3EIbxZEPmFCCCACWe9jpSN6KHA2P&quot;&gt;Videos&lt;/a&gt;)&lt;/li&gt;

&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;Gaussian Processes&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;

&lt;li&gt;Gaussian Process and Uncertainty Quantification Summer School, 2020 (&lt;a href=&quot;http://gpss.cc/gpss20/program&quot;&gt;Lectures&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/playlist?list=PLZ_xn3EIbxZHynuWRdYp4WDtpKm5Xo9Ge&quot;&gt;Videos&lt;/a&gt;)&lt;/li&gt;

&lt;/ol&gt;
</description>
        <pubDate>Fri, 18 Sep 2020 09:16:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2020/09/Machine-Learning-Resources</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2020/09/Machine-Learning-Resources</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>Virtual Workshops</title>
        <description>&lt;p align=&quot;justify&quot;&gt;Since the COVID-19 lockdown, we have had to adapt to a completely new paradigm, that is, 'working from home'. Although not encouraged prior to this pandemic, I have personally found that I have been quite efficient at multi-tasking. In this post, I will focus on the virtual workshops which I have attended (or will attend).  &lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;u&gt;ICLR 2020&lt;/u&gt; (26&lt;sup&gt;th&lt;/sup&gt; April to 1&lt;sup&gt;st&lt;/sup&gt; May, &lt;a href=&quot;https://iclr.cc/virtual_2020/index.html&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;)&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;I had the opportunity to present my work on Weak Lensing, Compression and Gaussian Processes (see &lt;a href=&quot;https://slideslive.com/38926202/gaussian-processes-emulator-and-moped-for-weak-lensing&quot;&gt;here&lt;/a&gt;) at the ICLR 2020. Apart from my work, it was quite inspiring to see how Machine Learning has developed from a purely scientific field to an engineering field. Today, it encompasses almost all branch of Science and Engineering. One of the main focus was on Climate Change, which is undeniably one of the hardest problems that humanity is currently facing.&lt;/p&gt;
  &lt;li&gt;&lt;u&gt;MLSS 2020&lt;/u&gt; (28&lt;sup&gt;th&lt;/sup&gt; June to 10&lt;sup&gt;th&lt;/sup&gt; July, &lt;a href=&quot;http://mlss.tuebingen.mpg.de/2020/schedule.html&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;)&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;Although I did not register for this workshop, we all had the opportunity to follow all talks live on Youtube. In particular, since my work is closely related to Bayesian Analysis, I was inspired by &lt;a href=&quot;https://shakirm.com/&quot;&gt;Shakir&lt;/a&gt;'s talks (Bayesian Inference I and II). Another topic which got me thinking was Meta-Learning by &lt;a href=&quot;
    https://www.stats.ox.ac.uk/~teh/&quot;&gt;Prof. Yee Whye Teh&lt;/a&gt; in which he refers to the following:&lt;/p&gt;
  &lt;blockquote&gt;
    &lt;p align=&quot;justify&quot;&gt;&lt;small&gt;&lt;i&gt;&quot;Our training procedure is based on a simple machine learning principle: test and train
      conditions must match&quot;&lt;/i&gt; -
      &lt;a href=&quot;https://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning&quot;&gt;Vinyals et al. 2016&lt;/a&gt;&lt;/small&gt;
    &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;li&gt;&lt;u&gt;ICML 2020&lt;/u&gt; (12&lt;sup&gt;th&lt;/sup&gt; July to 18&lt;sup&gt;th&lt;/sup&gt; July, &lt;a href=&quot;https://icml.cc/virtual/2020&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;)&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;We had a plethora of talks, workshops and tutorials for ICML 2020. I was particularly focused on the Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models workshop (&lt;a href=&quot;https://icml.cc/virtual/2020/workshop/5742&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;) and we had an interesting talk on how deep learning techniques are being used in Science by &lt;a href=&quot;https://en.wikipedia.org/wiki/Kyle_Cranmer&quot;&gt;Kyle Cranmer&lt;/a&gt;. Moreover, I also followed the nice tutorial on Bayesian Deep Learning and a Probabilistic Perspective of Model Construction (&lt;a href=&quot;https://icml.cc/virtual/2020/tutorial/5750&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;) by &lt;a href=&quot;https://cims.nyu.edu/~andrewgw/&quot;&gt;Andrew Wilson.&lt;/a&gt; We also had the opportunity to participate in various mentoring sessions (career advice, rising topics in Machine Learning, equality and many more) which are enormously helpful as a student.&lt;/p&gt;
  &lt;li&gt;&lt;u&gt;OxML 2020&lt;/u&gt; (17&lt;sup&gt;th&lt;/sup&gt; August to 25&lt;sup&gt;th&lt;/sup&gt; August, &lt;a href=&quot;https://www.oxfordml.school/&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;)&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;We started this workshop with two important lectures which were related to my own research. The two lectures were on Bayesian Machine Learning by Cheng Zhang and Gaussian Processes by James Hensman. Moreover we had a series of other interesting lectures on Neural Networks, Natural Language Processing (NLP), Computer Vision, Representation Learning, Causal Machine Learning, Reinforcement Learning and many more. Along wih these lectures, we had various tutorials and unconference sessions where participants themselves took the initiative to discuss a specific topic in depth.&lt;/p&gt; 
  &lt;li&gt;&lt;u&gt;GPSS 2020&lt;/u&gt; (14&lt;sup&gt;th&lt;/sup&gt; September to 17&lt;sup&gt;th&lt;/sup&gt; September, &lt;a href=&quot;
    http://gpss.cc/gpss20/&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;)&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;We had a series of lectures on various aspects of Gaussian Processes (GP), such as Scalable GP, Deep GP and various other lectures related to, for example, Bayesian Optimisation, Kernel design, Bayesian Neural Network, Composite GPs and many more. Importantly, &lt;a href=&quot;http://carlhenrik.com/&quot;&gt;Carl Henrik&lt;/a&gt; gave an interesting introduction to GPs and one funny, yet thought-provoking quote in one of his slides was that:&lt;/p&gt; 

  &lt;blockquote&gt;
    &lt;p align=&quot;justify&quot;&gt;&lt;small&gt;&lt;i&gt;Deep Learning is a bit like smoking, you know that it's wrong but you do it anyway because you want to look cool.&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
  &lt;/blockquote&gt;

&lt;!--   &lt;li&gt;&lt;u&gt;NeurIPS 2020&lt;/u&gt; (6&lt;sup&gt;th&lt;/sup&gt; December to 12&lt;sup&gt;th&lt;/sup&gt; December, &lt;a href=&quot;https://nips.cc/Conferences/2020&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;)&lt;/li&gt;
 --&gt;
&lt;/ol&gt;

&lt;!-- 
&lt;p align=&quot;justify&quot;&gt;This would not have been possible if I were to be physically present, simply because the latter cost at least £200 while with the virtual workshops, it costs no more than £50. I understand that it is not the same as being part of an actual workshop but I personally think I have been able to make the most out of them.&lt;/p&gt; --&gt;

</description>
        <pubDate>Fri, 10 Jul 2020 23:04:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2020/07/Virtual-Workshops</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2020/07/Virtual-Workshops</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>Research Paper</title>
        <description>&lt;p align=&quot;justify&quot;&gt;In this post, I will go through the steps I have personally found useful to do my research and lead the latter towards a publication. In general, the first paper (as first author) is always the most challenging part. It is through this process that I have learned how to think critically and to answer questions such as 'why is this research important?', 'where is it going to be used', 'how is it different from previous work?', 'what are the new contributions to the field as a whole?', 'how do I manage my time properly?' and various others.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;&lt;b&gt;Project definition&lt;/b&gt;&lt;/li&gt;
	&lt;p align=&quot;justify&quot;&gt;In the very first step, we normally start with a project definition. However, there is always an element of risk when defining a project. Some friends of mine have confessed that their projects did not seem to be promising after spending a few months on them.&lt;/p&gt;	
	&lt;li&gt;&lt;b&gt;Brainstorming&lt;/b&gt;&lt;/li&gt;
	&lt;p align=&quot;justify&quot;&gt;To avoid pitfalls (as we discussed in the first point), I highly recommend having brainstorming sessions with your collaborators and identify possible risks and limitations.&lt;/p&gt;
	&lt;blockquote&gt;
	&lt;p align=&quot;justify&quot;&gt;&lt;small&gt;&lt;i&gt;&quot;If I had an hour to solve a problem I’d spend 55 minutes thinking about the problem and five minutes thinking about solutions.&quot;&lt;/i&gt; - Albert Einstein&lt;/small&gt;&lt;/p&gt;
	&lt;/blockquote&gt;
	&lt;li&gt;&lt;b&gt;Identify a main reference paper&lt;/b&gt;&lt;/li&gt;
	&lt;p align=&quot;justify&quot;&gt;In general, finding a main reference paper is fundamental to the whole process. This implies understanding the paper, Mathematics and be able to implement (code) at least part of the paper.&lt;/p&gt;
	&lt;li&gt;&lt;b&gt;Fill in the gaps&lt;/b&gt;&lt;/li&gt;
	&lt;p align=&quot;justify&quot;&gt;At this point, we typically have a blurry image of how to channel the main idea (project), a large part due to missing knowledge about a specific topic. It would be very helpful to master certain topics (through textbooks, notes, internet, Youtube, Github) in order to fill in the gaps.&lt;/p&gt;
	&lt;li&gt;&lt;b&gt;Experiments&lt;/b&gt;&lt;/li&gt;
	&lt;p align=&quot;justify&quot;&gt;This is probably the most important part in the entire process, where we will be writing and testing our codes. This step might seem to be daunting for we will encounter many failures. However,&lt;/p&gt;	
	&lt;blockquote&gt;
	&lt;p align=&quot;justify&quot;&gt;&lt;small&gt;&lt;i&gt;&quot;Failure provides the opportunity to begin again, more intelligently.&quot;&lt;/i&gt; - Henry Ford&lt;/small&gt;&lt;/p&gt;
	&lt;/blockquote&gt;
	&lt;li&gt;&lt;b&gt;Updates&lt;/b&gt;&lt;/li&gt;
	&lt;p align=&quot;justify&quot;&gt;	Crucially, it would be nice to simultaneously document our code and write a draft paper as we are working. This allows us to keep track of everything we have been working on since the start of the project.&lt;/p&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;The Paper&lt;/b&gt;&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;Different papers have different format. It is important to first check and work with the right format in the very beginning itself to maximise efficiency. Below are some brief suggestions I received from my supervisors:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;u&gt;Abstract&lt;/u&gt;: Summarise key findings. Numbers are essential.&lt;/li&gt;
	&lt;li&gt;&lt;u&gt;Introduction&lt;/u&gt;: Motivate the study.&lt;/li&gt;
	&lt;li&gt;&lt;u&gt;Body&lt;/u&gt;: Elaborate on the theory, data, methods and results.&lt;/li&gt;
	&lt;li&gt;&lt;u&gt;Conclusion&lt;/u&gt;: Remind the reader what the paper is about and explain briefly that the aims have been met.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;Most Important Suggestions&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Keep in touch with your supervisors (emails and weekly meetings).&lt;/li&gt;
	&lt;li&gt;Have at least one mentor - I have found it easier to speak to my collaborator who is a Research Fellow.&lt;/li&gt;
	&lt;li&gt;Always make notes - we are in a world with a wealth of information. It is important to channel ideas and information.&lt;/li&gt;
	&lt;li&gt;Do not stick with yourself. In other words, do not pigeonhole yourself.&lt;/li&gt;
	&lt;li&gt;Ask questions - there is no stupid question.&lt;/li&gt;
	&lt;blockquote&gt;
	&lt;p align=&quot;justify&quot;&gt;&lt;small&gt;&lt;i&gt;&quot;If you ask a stupid question, you may feel stupid; if you don't ask a stupid question, you remain stupid.&quot;&lt;/i&gt; - Tony Rothman&lt;/small&gt;&lt;/p&gt;
	&lt;/blockquote&gt;
	&lt;li&gt;Understanding is key. Some pieces of work are rather engineering but we should be able to explain the concept behind.&lt;/li&gt;
	&lt;li&gt;Acknowledge that we do not understand something and as we seek knowledge/help/support, we shall receive.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 10 Jul 2020 19:08:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2020/07/Research-Paper</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2020/07/Research-Paper</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>Black Lives Matter Protests</title>
        <description>&lt;p align=&quot;justify&quot;&gt;Since the death of George Floyd on the 25&lt;sup&gt;th&lt;/sup&gt; May 2020 (see article on &lt;a href=&quot;https://en.wikipedia.org/wiki/Killing_of_George_Floyd&quot;&gt;Wikipedia&lt;/a&gt;), there has been ongoing protest not only in the US but all over the world, where people are expressing their concerns about systemic racism and unfair injustices on Black people. Growing up in Mauritius, which is a pluri-cultural, multi-lingual and multi-ethinic society, I have a deep feeling and connection with this issue and hopefully, through this post, I will put forward some of my views on this.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;It goes without saying that our past will define our future, and indeed, in the Mauritian context, despite the fact that our ancestors had been subject to slavery (&lt;a href=&quot;https://en.wikipedia.org/wiki/History_of_Mauritius&quot;&gt;History of Mauritius&lt;/a&gt;), today, we are enjoying the full freedom of religion, press, speech and many more. This makes us one of the highly ranked countries where these are generously acceptable. That said, I am not at all claiming that my island is a paradise, which is often the outlook at the international level. We still have much work to do, in order to combat, for example, poverty, crime, corruption and many more. I am almost certain that these form part of every countries' issues that need to be addressed.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;My next journey was in Cape Town where I spent two years for my MSc. As we all know, South Africa went through the Apartheid and I could still feel the tension among the people. Sadly, when I was there, students from the University of Cape Town were protesting for the Rhodes memorial to be removed (&lt;a href=&quot;https://en.wikipedia.org/wiki/Rhodes_Must_Fall&quot;&gt;Rhodes Must Fall&lt;/a&gt;). On Facebook, Black people were constantly being referred to as 'Black Monkeys' and this had an impact on me. Are we living in a society where people are pretending everything is okay? On the other hand, I remember a friend telling me, 'you don't know what those people did to our parents and grand-parents'. Another friend told me that his grand-dad had to live with a bullet in his face because the doctor said that if it were to be removed, he would have passed away.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;I am surprised that we often respond according to the 'action-reaction' principle (Newton's first law: for every action there will be an equal and opposite reaction). If we did not have the protests related to the killing of Black people, then maybe there would have been no (re)action. My question is, why do we even have to call people Black, White or Coloured? Why do we need to associate a colour based on our skin colour? Since I come from Mauritius, I do not even know what my ethnic background is and this often gets asked when filling in forms. On the flip side, I am glad that various actions have been adopted since the protests and two which I am aware of are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The Presidential Scholarship for Black Students (&lt;a href=&quot;https://www.imperial.ac.uk/giving/donate/black-scholarship/&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Google commitments to racial equity (&lt;a href=&quot;https://www.blog.google/inside-google/company-announcements/commitments-racial-equity/&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;justify&quot;&gt;I personally think that we can do more to alleviate this issue of (systemic) racism in our society. I am certain that many people from Africa do not know how to leverage these opportunities. They need to be sensitised about the process and make them feel they have a place of belonging. I am grateful to all my colleagues that we now have sessions dedicated to BAME (Black Asian Minority Ethnic). I strongly believe that education is the main pillar towards building a fair and equal society. If a department (let us consider, for simplicity, just 100 universities from Europe and US) takes the initiative to fund at least 1 student from Africa every year for a time lapse of 10 years, then at the end of the process, we will have at least 1000 students who have had access to world-class education. I remember a saying from my parents, which essentially says, 'be a light that can never be extinguished' and I strongly opine that it's only education which enables one to be someone in life.&lt;/p&gt;

</description>
        <pubDate>Wed, 10 Jun 2020 16:30:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2020/06/Black-Lives-Matter</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2020/06/Black-Lives-Matter</guid>
        
        
        <category>Social</category>
        
      </item>
    
      <item>
        <title>Iterative Method For Likelihood Emulation</title>
        <description>&lt;p align=&quot;justify&quot;&gt;The paper, entitled '&lt;b&gt;&lt;font size=&quot;2.5&quot;&gt;Cosmological parameter estimation via iterative emulation of likelihoods&lt;/font&gt;&lt;/b&gt;' was published on &lt;a href=&quot;https://arxiv.org/abs/1912.08806&quot;&gt;arXiv&lt;/a&gt; recently. The idea behind is to use Gaussian Process to emulate the log-likelihood and to progressively augment the training set via Bayesian Optimisation. In this post, I shall illustrate the technique via simple straight line fitting example, which I believe will be trivial to follow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/bo.png&quot; align=&quot;right&quot; width=&quot;400&quot; style=&quot;margin-left: 10px; margin-bottom: 10px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;font size=&quot;3&quot;&gt;Analytical Posterior&lt;/font&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;To start with, we randomly draw 50 points uniformly from $x\in[0, 1]$ and compute $\mathbf{y}$, which is given by

$$
\mathbf{y} = \theta\mathbf{x} + \boldsymbol{\epsilon}
$$
&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;We fix $\theta = 1$ and $\boldsymbol{\epsilon}\sim\mathcal{N}(0, 0.04^{2})$. We also assume a Gaussian prior for $\theta$, where $p(\theta)=\mathcal{N}(1,1)$. The posterior distribution of $\theta$ can be derived analytically and is in fact another Gaussian distribution, given by:
$$
p(\theta|\mathbf{y}) = \mathcal{N}(\mu, \sigma^{2})
$$

where 

&lt;ul&gt;
  &lt;li&gt;$\mu = 625\sigma^{2}\mathbf{D}^{\textrm{T}}\mathbf{y}$&lt;/li&gt;
  &lt;li&gt;$\sigma^{2}=(1 + 625\mathbf{D}^{\textrm{T}}\mathbf{D})^{-1}$&lt;/li&gt;
  &lt;li&gt;$\mathbf{D} = [x_{0},\,x_{1},\ldots x_{N-1}]^{\textrm{T}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;with the factor 625 arising due to 1/0.04&lt;sup&gt;2&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;font size=&quot;3&quot;&gt;Gaussian Process and Bayesian Optimization&lt;/font&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Gaussian Process is a joint multivariate Gaussian distribution over functions with a continuous domain. We will not cover it in this post (for further details, see &lt;a href=&quot;https://harry45.github.io/blog/2016/10/Gaussian-Process&quot;&gt;here&lt;/a&gt;). The predictive distribution is a Normal distribution with mean and variance given respectively by:

$$
f_{*} = \mathbf{k}_{*}^{\textrm{T}}\mathbf{K}^{-1}\mathbf{y}_{\textrm{train}}
$$

$$
\sigma_{*}^{2} = k_{**} - \mathbf{k}_{*}^{\textrm{T}}\mathbf{K}^{-1}\mathbf{k}_{*}
$$

On the other hand, Bayesian Optimization is a proxy for finding the optima of expensive objective functions via the acquisition function, which we will discuss briefly below. A typical example of expensive computation is in cosmology where we have a series of multiple integrations and other expensive computations (for example, when we also have to account for the systematics) prior to computing the log-likelihood in a Bayesian analysis.

&lt;p&gt;&lt;b&gt;&lt;font size=&quot;2&quot;&gt;Acquisition Functions&lt;/font&gt;&lt;/b&gt;&lt;/p&gt;
One class of acquisition function (also referred to infill function) is improvement-based function. &lt;i&gt;Probability of improvement&lt;/i&gt; (also called maximum probability of improvement or the P-algorithm) is one such example and is given by:

$$
\textrm{PI}(\theta) = \Phi(z)
$$

In the same spirit, the &lt;i&gt;expected improvement&lt;/i&gt; does not only consider the probability of improvement but also takes into account the magnitude of improvement that the additional point will provide. It reads:

$$
\textrm{EI}(\theta)=\begin{cases}
\begin{array}{c}
\left(\mu-f_{\textrm{max}}-\alpha\right)\Phi(z)+\sigma\phi(z)\\
0
\end{array} &amp;amp; \begin{array}{c}
\textrm{if }\sigma&amp;gt;0\\
\textrm{if }\sigma=0
\end{array}\end{cases}
$$

where $\Phi(\centerdot)$ and $\phi(\centerdot)$ correspond to the Cumulative Distribution Function (CDF) and Probability Distribution Function (PDF) of a normal distribution respectively. $f_\textrm{max}$ refers to the maximum of the expensive function, given a set of inputs, $\theta$ and 

$$
z(\theta) = \frac{\mu(\theta) - f_{\textrm{max}} - \alpha}{\sigma(\theta)}.
$$

On the other hand, another acquisition function based on upper confidence bound is given by:

$$
\textrm{UCB}(\theta) = \mu + \alpha \sigma
$$

Note the additional parameter $\alpha$ in the acquisition functions. It is usually user-defined and it essentially controls the trade-off between exploration (region of high uncertainty) and exploitation (region with high mean).
&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;font size=&quot;3&quot;&gt;Our Implementation&lt;/font&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;
We initially start with just 4 training points (generated using Latin Hypercube samples, &lt;a href=&quot;https://en.wikipedia.org/wiki/Latin_hypercube_sampling&quot;&gt;LHS&lt;/a&gt;) given by the first 4 rows in the table at the bottom. We then use the Upper Confidence Bound (UCB) acquisition function (with $\alpha=15$) to iteratively add two points (the last two rows in red in the table) to the Gaussian Process model. See algorithm below for further details. 
&lt;/p&gt;

&lt;style&gt;
table {width: 30%;}
&lt;/style&gt;

&lt;table class=&quot;tableizer-table&quot; align=&quot;left&quot;&gt;
&lt;thead&gt;&lt;tr class=&quot;tableizer-firstrow&quot;&gt;&lt;th&gt;$\theta$ &lt;/th&gt;&lt;th&gt;$\textrm{log } L$&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;0.9662&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;-26.1204&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;1.0064&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;-17.2318&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;1.0223&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;-18.1605&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;1.0511&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;-26.2248&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&lt;font color=&quot;red&quot;&gt;0.9860&lt;/font&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;&lt;font color=&quot;red&quot;&gt;-19.7343&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&lt;font color=&quot;red&quot;&gt;1.0369&lt;/font&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;&lt;font color=&quot;red&quot;&gt;-21.2069&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/images/BO-Algorithm.png&quot; align=&quot;right&quot; width=&quot;600&quot; style=&quot;margin-right: 10px; margin-bottom: 10px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;font size=&quot;3&quot;&gt;Results and Conclusions&lt;/font&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;
In this setup, we are able to reconstruct the log-likelihood almost perfectly after augmenting the data set in two iterations. As seen in the plot below in the right panel, the posterior distribution of $\theta$ is identical to the exact one (which we calculated analytically previously). The vertical broken line shows the value of $\theta=1$ which we initially used to generate the data. 
&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/finalPosterior.png&quot; align=&quot;center&quot; width=&quot;800&quot; style=&quot;margin-bottom: 0.1px&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;
However, in high dimensions, the volume of the parameter space increases and it is not easy to reconstruct a function perfectly (if this is the main objective). Moreover, the acquisition functions themselves contain multiple local optima (as seen in the Figure on the top) and the choice of the acquisition function is an interesting topic. They can often be greedy and favour exploitation over exploration, hence the choice of $\alpha$ also matters. 
&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;font size=&quot;3&quot;&gt;References&lt;/font&gt;&lt;/b&gt;&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;A Tutorial on Bayesian Optimization (arXiv, &lt;a href=&quot;https://arxiv.org/abs/1012.2599&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;) &lt;/li&gt;
&lt;li&gt;Cosmological parameter estimation via iterative emulation of likelihoods (arXiv, &lt;a href=&quot;https://arxiv.org/abs/1912.08806&quot;&gt;&lt;i style=&quot;font-size:12px&quot; class=&quot;fa&quot;&gt;&amp;#xf08e;&lt;/i&gt;&lt;/a&gt;) &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 10 Mar 2020 15:30:00 +0000</pubDate>
        <link>http://localhost:4000/blog/2020/03/Iterative-Method-For-Likelihood-Emulation</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2020/03/Iterative-Method-For-Likelihood-Emulation</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>Emerging trends in AI and online education</title>
        <description>&lt;p align=&quot;justify&quot;&gt;On this day, &lt;a href=&quot;https://www.coursera.org/instructor/andrewng&quot;&gt;Professor Andrew Ng&lt;/a&gt; was at Imperial College London. He was not only there as a speaker for the (&lt;a href=&quot;https://www.imperial.ac.uk/data-science/&quot;&gt;Data Science Institute, DSI&lt;/a&gt;) Distinguished Lecture programme but to also commemorate the 5&lt;sup&gt;th&lt;/sup&gt; year anniversary of the existence of the DSI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/andrew-ng.jpg&quot; align=&quot;left&quot; width=&quot;400&quot; style=&quot;margin-right: 10px; margin-bottom: 10px&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;It was not a big suprise to see the full lecture theatre completely full of postgraduate students, vehement to hear from Prof. Andrew. Somewhat surprising though, was the fact that Prof. Andrew came in with a yellow notebook rather than a laptop as we were all expecting some sort of digital presentation. In fact, he chose to speak to the audience directly and use the white board to write a few important points. To me, this simple act, in itself, is an illustration of a true leader. He started off by speaking about his teaching courses at Stanford University. I personally think he is doing a fantastic job at revolutionalizing teaching methods. In particular, he refers to his own teaching system where students watch videos before coming to the class, and projects/ideas are discussed during teaching sessions. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;	On the same note, since he is also trying to automate grading system as well, teaching and research assistants spend more time discussing projects/ideas with students. Other important topics discussed include but not limited to:&lt;/p&gt;

&lt;ul&gt;
  
  &lt;li&gt;&lt;u&gt;ANI (Artificial Narrow Intelligence)&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;Up to now, there has been major development in supervised learning, where an algorithm is fed with thousands, possibly millions of examples before one tests on unseen examples. This form of Machine Learning is at the forefront of most technological application these days.&lt;/p&gt;
  
  &lt;li&gt;&lt;u&gt;AGI (Artificial General Intelligence)&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;On the other hand, we are also far from achieving AGI - a state where an agent will be able to take decision on its own. Clearly, the human brain is a complicated system and although much work has been done in the Neuroscience research field, having a full understanding of how the brain works, remains a delicate topic of research.&lt;/p&gt;
  
  &lt;li&gt;&lt;u&gt;Small Data&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;Another interesting aspect of the discussion was about small data. Machine Learning algorithms are data crunching in general. It is very hard to work with small data set. As an example, if we have access to millions of medical images, we can do a good job at predicting whether a patient is suffering from, say, cancer. What would we do if we had only 10 images?&lt;/p&gt;
  
  &lt;li&gt;&lt;u&gt;Generalizability&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;Algorithms are in general very specific, for example, if an algorithm is designed to predict the market value of an object, it is very less likely to be used out of the box for an almost similar problem. Although, methods such as domain adaptation and transfer learning are currently being explored, Prof. Andrew believes that we are still far from building generalizable algorithm.&lt;/p&gt;
   
  &lt;li&gt;&lt;u&gt;Jobs&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;Job security is a heated topic of discussion. While some people believe that people will be jobless as robots will take over our jobs, others believe that there will be a radical change in the job type itself. For example, Prof. Andrew says that there will be jobs where half of the work might be done by the robot but the constant presence of the human is still required. One funny example which Prof. Andrew gave was that we are still far from having a robot which will be able to give us a perfect hair cut!&lt;/p&gt;

  &lt;li&gt;&lt;u&gt;Education&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;Moreover, education matters! If we want to move forward and embrace AI, we have to develop our skill sets in such a way that we will be able to survive a rapidly changing environment. Education allows us to harness the talents of brilliant people and a global online education system not only promotes learning but also enables people who cannot afford university fees to access world class learning materials.&lt;/p&gt;
  
  &lt;li&gt;&lt;u&gt;Creativity&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;One question from the audience was: what is creativity? Prof. Andrew humbly said, 'I don't know the definition of creativity'. He further went on telling on that when his group publishes a paper, people say that this was a creative piece of work. However, he admitted that the work is actually quite dirty in the lab, especially when working with deep neural network where one has to build the 'right' architecture and do hyperparameter tuning well.&lt;/p&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 01 Apr 2019 07:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2019/04/Andrew-Ng-Visit</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2019/04/Andrew-Ng-Visit</guid>
        
        
        <category>Workshop and Conference</category>
        
      </item>
    
      <item>
        <title>Methods for Statistical Inference (Paris)</title>
        <description>&lt;dl class=&quot;wp-caption alignright&quot; style=&quot;max-width: 350px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/ihp.jpg&quot; alt=&quot;Institut Henri Poincaré&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;Institut Henri Poincaré&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;I had the opportunity to attend the Methods for Statistical Inference school, which was organised from the 22&lt;sup&gt;nd&lt;/sup&gt; to the 26&lt;sup&gt;th&lt;/sup&gt; October 2018 at the Institut Henri Poincaré, Paris. In particular, the goal of the school was not only to foster collaboration but to also understand if we have the right tools for cosmology over the next decade. In particular, the focus was also around extracting maximum cosmological information from data and identify new techniques and methods to go about addressing this topic.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;There was a series of interesting talks (around 25 talks), related to Bayesian techniques and Machine Learning. The ones which are related to the topics I am particularly interested were:&lt;/p&gt;

&lt;ol type=&quot;1&quot;&gt;

&lt;li&gt;
Hierarchical modelling of weak lensing and photometric redshifts
&lt;/li&gt;

&lt;li&gt;
Bayesian optimisation for likelihood-free cosmological inference
&lt;/li&gt;

&lt;li&gt;
Bayesian analysis of astronomical catalogues, with application to measuring the Hubble constant using binary neutron stars
&lt;/li&gt;

&lt;li&gt;
Deep Learning architectures for the estimation of photometric redshifts of galaxies and the classification of light curves
&lt;/li&gt;

&lt;li&gt;
Promises and challenges of Deep Learning in Cosmology
&lt;/li&gt;

&lt;li&gt;
Posteriors and marginals in low and high dimensions with applications to large scale structure analysis
&lt;/li&gt;

&lt;li&gt;
Deep learning for science: steps to opening the pandora box
&lt;/li&gt;

&lt;li&gt;
Bayesian data interpretation with large scale cosmological models
&lt;/li&gt;

&lt;li&gt;
Machine learning based statistical inference
&lt;/li&gt;

&lt;li&gt;
Learning Multiscale Physics with Deep Neural Networks
&lt;/li&gt;

&lt;/ol&gt;
</description>
        <pubDate>Tue, 30 Oct 2018 20:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/2018/10/Methods-for-Statistical-Inference-Paris</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2018/10/Methods-for-Statistical-Inference-Paris</guid>
        
        
        <category>Workshop and Conference</category>
        
      </item>
    
      <item>
        <title>Data Science Summer School 2018 (Paris)</title>
        <description>&lt;p align=&quot;justify&quot;&gt;The &lt;a href=&quot;https://2018.ds3-datascience-polytechnique.fr/&quot;&gt;Data Science Summer School&lt;/a&gt; was organised from the 25&lt;sup&gt;th&lt;/sup&gt; to the 29&lt;sup&gt;th&lt;/sup&gt; June 2018 at the École Polytechnique. The school had a broad spectrum of topics covered and the presence of Prof. Yann Lecun, leading expert in the deep learning and Cédric Villani, winner of the Fields medal in 2010, did not go unnoticed.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;I will briefly touch upon the talks which I attended. All the session on Monday were mostly related to the general non-technical issues around Machine Learning and we had the following lectures:&lt;/p&gt;

&lt;ol type=&quot;1&quot;&gt;

&lt;li&gt;
Deep Learning
&lt;/li&gt;
&lt;li&gt;
Trust and Transparency
&lt;/li&gt;
&lt;li&gt;
Fairness in Machine Learning
&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;justify&quot;&gt;On Tuesday and Wednesday, we had more technical talks in:&lt;/p&gt;

&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;
Machine Learning Research Design and the GDPR
&lt;/li&gt;
&lt;li&gt;
Probabilistic Numerical Computation: A Role for Statisticians in Numerical Analysis?
&lt;/li&gt;
&lt;li&gt;
Deep Learning for Medical Imaging and Precision Medicine
&lt;/li&gt;
&lt;li&gt;
Deep Latent Variable Models in Medical Informatics
&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;justify&quot;&gt;On Thursday and Friday, we had tutorials and I chose the following two:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;
Representing and Comparing Probabilities with Kernels
&lt;/li&gt;
&lt;li&gt;
Crash Course in Deep Learning and Pytorch
&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 30 Jun 2018 09:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2018/06/Data-Science-Summer-School</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2018/06/Data-Science-Summer-School</guid>
        
        
        <category>Workshop and Conference</category>
        
      </item>
    
  </channel>
</rss>
