<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arrykrishna Mootoovaloo</title>
    <description>22, Derwentwater Road, Acton Town, London (W3 6DE)</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 28 Feb 2020 00:43:37 +0000</pubDate>
    <lastBuildDate>Fri, 28 Feb 2020 00:43:37 +0000</lastBuildDate>
    <generator>Jekyll v3.3.0</generator>
    
      <item>
        <title>Emerging trends in AI and online education</title>
        <description>&lt;p align=&quot;justify&quot;&gt;On this day, &lt;a href=&quot;https://www.coursera.org/instructor/andrewng&quot;&gt;Professor Andrew Ng&lt;/a&gt; was at Imperial College London. He was not only there as a speaker for the the (&lt;a href=&quot;https://www.imperial.ac.uk/data-science/&quot;&gt;Data Science Institute, DSI&lt;/a&gt;) Distinguished Lecture programme but to also commemorate the 5th year anniversary of the existence of the DSI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/andrew-ng.jpg&quot; align=&quot;left&quot; width=&quot;400&quot; style=&quot;margin-right: 10px; margin-bottom: 10px&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;It was not a big suprise to see the full lecture theatre completely full of postgraduate students, vehement to hear from Prof. Andrew. Somewhat surprising though, was the fact that Prof. Andrew came in with a yellow notebook rather than a laptop as we were all expecting some sort of digital presentation. In fact, he chose to speak to the audience directly and use the white board to write a few important points. To me, this simple act, in itself, is an illustration of a true leader. He started off by speaking about his teaching courses at Stanford University. I personally think he is doing a fantastic job at revolutionalizing teaching methods. In particular, he refers to his own teaching system where students watch videos before coming to the class, and projects/ideas are discussed during teaching sessions. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;	On the same note, since he is also trying to automate grading system as well, teaching and research assistants spend more time discussing projects/ideas with students. Other important topics discussed include but not limited to:&lt;/p&gt;

&lt;ul&gt;
  
  &lt;li&gt;&lt;u&gt;ANI (Artificial Narrow Intelligence)&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;Up to now, there has been major development in supervised learning, where an algorithm is fed with thousands, possibly millions of examples before one tests on unseen examples. This form of Machine Learning is at the forefront of most technological application these days.&lt;/p&gt;
  
  &lt;li&gt;&lt;u&gt;AGI (Artificial General Intelligence)&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;On the other hand, we are also far from achieving AGI - a state where an agent will be able to take decision on its own. Clearly, the human brain is a complicated system and although much work has been done in the Neuroscience research field, having a full understanding of how the brain works, remains a delicate topic of research.&lt;/p&gt;
  
  &lt;li&gt;&lt;u&gt;Small Data&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;Another interesting aspect of the discussion was about small data. Machine Learning algorithms are data crunching in general. It is very hard to work with small data set. As an example, if we have access to millions of medical images, we can do a good job at predicting whether a patient is suffering from, say, cancer. What would we do if we had only 10 images?&lt;/p&gt;
  
  &lt;li&gt;&lt;u&gt;Generalizability&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;Algorithms are in general very specific, for example, if an algorithm is designed to predict the market value of an object, it is very less likely to be used out of the box for an almost similar problem. Although, methods such as domain adaptation and transfer learning are currently being explored, Prof. Andrew believes that we are still far from building generalizable algorithm.&lt;/p&gt;
   
  &lt;li&gt;&lt;u&gt;Jobs&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;Job security is a heated topic of discussion. While some people believe that people will be jobless as robots will take over our jobs, others believe that there will be a radical change in the job type itself. For example, Prof. Andrew says that there will be jobs where half of the work might be done by the robot but the constant presence of the human is still required. One funny example which Prof. Andrew gave was that we are still far from having a robot which will be able to give us a perfect hair cut!&lt;/p&gt;

  &lt;li&gt;&lt;u&gt;Education&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;Moreover, education matters! If we want to move forward and embrace AI, we have to develop our skill sets in such a way that we will be able to survive a rapidly changing environment. Education allows us to harness the talents of brilliant people and a global online education system not only promotes learning but also enables people who cannot afford university fees to access world class learning materials.&lt;/p&gt;
  
  &lt;li&gt;&lt;u&gt;Creativity&lt;/u&gt;&lt;/li&gt;
  &lt;p align=&quot;justify&quot;&gt;One question from the audience was: what is creativity? Prof. Andrew humbly said, 'I don't know the definition of creativity'. He further went on telling on that when his group publishes a paper, people say that this was a creative piece of work. However, he admitted that the work is actually quite dirty in the lab, especially when working with deep neural network where one has to build the 'right' architecture and do hyperparameter tuning well.&lt;/p&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 01 Apr 2019 07:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2019/04/Andrew-Ng-Visit</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2019/04/Andrew-Ng-Visit</guid>
        
        
        <category>Workshop and Conference</category>
        
      </item>
    
      <item>
        <title>CLASS and MontePython Workshop (Cambridge)</title>
        <description>&lt;dl class=&quot;wp-caption alignleft&quot; style=&quot;max-width: 350px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/cambridge.jpg&quot; alt=&quot;University of Cambridge&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;University of Cambridge&lt;/dd&gt;
&lt;/dl&gt;
</description>
        <pubDate>Thu, 15 Nov 2018 18:30:00 +0000</pubDate>
        <link>http://localhost:4000/blog/2018/11/Cambridge-CLASS-MontePython</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2018/11/Cambridge-CLASS-MontePython</guid>
        
        
        <category>Workshop and Conference</category>
        
      </item>
    
      <item>
        <title>Methods for Statistical Inference (Paris)</title>
        <description>&lt;dl class=&quot;wp-caption alignright&quot; style=&quot;max-width: 350px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/ihp.jpg&quot; alt=&quot;Institut Henri Poincaré&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;Institut Henri Poincaré&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;I had the opportunity to attend the Methods for Statistical Inference school, which was organised from the 22&lt;sup&gt;nd&lt;/sup&gt; to the 26&lt;sup&gt;th&lt;/sup&gt; October 2018 at the Institut Henri Poincaré, Paris. In particular, the goal of the school was not only to foster collaboration but to also understand if we have the right tools for cosmology over the next decade. In particular, the focus was also around extracting maximum cosmological information from data and identify new techniques and methods to go about addressing this topic.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;There was a series of interesting talks (around 25 talks), related to Bayesian techniques and Machine Learning. The ones which are related to the topics I am particularly interested were:&lt;/p&gt;

&lt;ol type=&quot;1&quot;&gt;

&lt;li&gt;
Hierarchical modelling of weak lensing and photometric redshifts
&lt;/li&gt;

&lt;li&gt;
Bayesian optimisation for likelihood-free cosmological inference
&lt;/li&gt;

&lt;li&gt;
Bayesian analysis of astronomical catalogues, with application to measuring the Hubble constant using binary neutron stars
&lt;/li&gt;

&lt;li&gt;
Deep Learning architectures for the estimation of photometric redshifts of galaxies and the classification of light curves
&lt;/li&gt;

&lt;li&gt;
Promises and challenges of Deep Learning in Cosmology
&lt;/li&gt;

&lt;li&gt;
Posteriors and marginals in low and high dimensions with applications to large scale structure analysis
&lt;/li&gt;

&lt;li&gt;
Deep learning for science: steps to opening the pandora box
&lt;/li&gt;

&lt;li&gt;
Bayesian data interpretation with large scale cosmological models
&lt;/li&gt;

&lt;li&gt;
Machine learning based statistical inference
&lt;/li&gt;

&lt;li&gt;
Learning Multiscale Physics with Deep Neural Networks
&lt;/li&gt;

&lt;/ol&gt;
</description>
        <pubDate>Tue, 30 Oct 2018 20:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/2018/10/Methods-for-Statistical-Inference-Paris</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2018/10/Methods-for-Statistical-Inference-Paris</guid>
        
        
        <category>Workshop and Conference</category>
        
      </item>
    
      <item>
        <title>Data Science Summer School 2018</title>
        <description>&lt;p align=&quot;justify&quot;&gt;The &lt;a href=&quot;https://2018.ds3-datascience-polytechnique.fr/&quot;&gt;Data Science Summer School&lt;/a&gt; was organised from the 25&lt;sup&gt;th&lt;/sup&gt; to the 29&lt;sup&gt;th&lt;/sup&gt; June 2018 at the École Polytechnique. The school had a broad spectrum of topics covered and the presence of Prof. Yann Lecun, leading expert in the deep learning and Cédric Villani, winner of the Fields medal in 2010, did not go unnoticed.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;I will briefly touch upon the talks which I attended. All the session on Monday were mostly related to the general non-technical issues around Machine Learning and we had the following lectures:&lt;/p&gt;

&lt;ol type=&quot;1&quot;&gt;

&lt;li&gt;
Deep Learning
&lt;/li&gt;
&lt;li&gt;
Trust and Transparency
&lt;/li&gt;
&lt;li&gt;
Fairness in Machine Learning
&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;justify&quot;&gt;On Tuesday and Wednesday, we had more technical talks in:&lt;/p&gt;

&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;
Machine Learning Research Design and the GDPR
&lt;/li&gt;
&lt;li&gt;
Probabilistic Numerical Computation: A Role for Statisticians in Numerical Analysis?
&lt;/li&gt;
&lt;li&gt;
Deep Learning for Medical Imaging and Precision Medicine
&lt;/li&gt;
&lt;li&gt;
Deep Latent Variable Models in Medical Informatics
&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;justify&quot;&gt;On Thursday and Friday, we had tutorials and I chose the following two:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;
Representing and Comparing Probabilities with Kernels
&lt;/li&gt;
&lt;li&gt;
Crash Course in Deep Learning and Pytorch
&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 30 Jun 2018 09:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2018/06/Data-Science-Summer-School</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2018/06/Data-Science-Summer-School</guid>
        
        
        <category>Workshop and Conference</category>
        
      </item>
    
      <item>
        <title>Yandex Machine Learning School 2018</title>
        <description>&lt;p align=&quot;justify&quot;&gt;The Yandex Machine Learning School was organised from the 16&lt;sup&gt;th&lt;/sup&gt; to 26&lt;sup&gt;th&lt;/sup&gt; January. It was a quite intensive school with a series of lectures, homework, assignments, tutorials and exam over the course of 10 days only. In particular, topics covered included the following:&lt;/p&gt;

&lt;ol type=&quot;1&quot;&gt;

&lt;li&gt;
Introduction to Machine Learning
&lt;/li&gt;

&lt;li&gt;
$k$-Nearest Neighbour
&lt;/li&gt;

&lt;li&gt;
Model Complexity
&lt;/li&gt;

&lt;li&gt;
Principle Component Analysis, PCA
&lt;/li&gt;

&lt;li&gt;
Singular Value Decomposition
&lt;/li&gt;

&lt;li&gt;
Linear Regression
&lt;/li&gt;

&lt;li&gt;
Linear Classification
&lt;/li&gt;

&lt;li&gt;
Kernel Trick
&lt;/li&gt;

&lt;li&gt;
Classifier Evaluation
&lt;/li&gt;

&lt;li&gt;
Decision Trees
&lt;/li&gt;

&lt;li&gt;
Ensemble Methods
&lt;/li&gt;

&lt;li&gt;
Boosting
&lt;/li&gt;

&lt;li&gt;
Neural Network
&lt;/li&gt;

&lt;li&gt;
Tricks and Philosophy in Deep Learning
&lt;/li&gt;

&lt;li&gt;
Deep Learning - Computer Vision Applications
&lt;/li&gt;

&lt;li&gt;
Deep Learning - Recurrent Neural Network
&lt;/li&gt;

&lt;li&gt;
Deep Learning - Recurrent Neural Network
&lt;/li&gt;

&lt;li&gt;
Reinforcement Learning in a Nutshell
&lt;/li&gt;

&lt;li&gt;
Generative Adversarial Networks and Autoencoders
&lt;/li&gt;

&lt;/ol&gt;

&lt;p align=&quot;justify&quot;&gt;All lectures and tutorials are found on &lt;a href=&quot;https://github.com/yandexdataschool/MLatImperial2018&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 27 Feb 2018 08:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/2018/02/Yandex-Machine-Learning</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2018/02/Yandex-Machine-Learning</guid>
        
        
        <category>Workshop and Conference</category>
        
      </item>
    
      <item>
        <title>Bayesian Evidence - The Gaussian Linear Model</title>
        <description>&lt;p align=&quot;justify&quot;&gt;Following our recent post on &lt;a href=&quot;https://harry45.github.io/blog/2017/05/Bayesian-Model-Selection&quot;&gt;Bayesian Model Selection&lt;/a&gt;, here we will illustrate it using a simple example of the Bayesian Evidence calculation using the &lt;a href=&quot;https://harry45.github.io/blog/2017/03/Linear-Regression&quot;&gt;Gaussian Linear Model&lt;/a&gt;. Consider the figure below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/TwoModels.jpg&quot; align=&quot;left&quot; width=&quot;410&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;We have our data, which is dominated by noise, $\mathbf{n}\sim\mathcal{N}\left(0,\,0.02\right)$. Here, we are motivated to consider two models, $\mathcal{M}_{1}$ and $\mathcal{M}_{2}$, given by

$$
y=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}+\theta_{4}x^{4}
$$

$$
y=\theta_{0}+\theta_{1}x+\theta_{4}x^{4}
$$

It is actually very hard to choose the best model just by looking at the fit. Note that the fitting method used here is the Maximum a Posteriori method, MAP where we assume the prior on each parameter is Gaussian distributed with mean 0 and variance 1, that is, $\theta_{i}\sim\mathcal{N}\left(0,\,1\right)$. Moreover, setting $\theta_{2}$ to zero reduces $\mathcal{M}_{1}$ to $\mathcal{M}_{2}$. This is a common scenario in various fitting problem we deal with in Statistics. If we naively perform a $\chi^{2}-$test, then the model with the larger number of parameters will always be selected. This is a case of overfitting we encounter and we should be cautious about it. The notation we will be using are: $\mathbf{D}$ as the design matrix and $\mathbf{P}$ as the covariance matrix for the priors. See &lt;a href=&quot;https://harry45.github.io/blog/2017/03/Linear-Regression&quot;&gt;here&lt;/a&gt; for further details on $\mathbf{D}$ and $\mathbf{b}$. The Bayesian Evidence is given by 

$$
\mathbb{Z}=\mathcal{P}\left(\mathcal{D}\left|\mathcal{M}\right.\right)=\int\mathcal{P}\left(\mathcal{D}\left|\boldsymbol{\theta},\,\mathcal{M}\right.\right)\mathcal{P}\left(\boldsymbol{\theta}\left|\mathcal{M}\right.\right)d\boldsymbol{\theta}
$$

$$
\mathbb{Z}=\left(\sqrt{\left|2\pi\mathbf{P}^{-1}\right|}\,{\prod_{i}}\sqrt{2\pi\sigma_{i}^{2}}\right)^{-1}\int\textrm{exp}\left[-\dfrac{1}{2}\left\{  \left(\mathbf{b}-\mathbf{D}\boldsymbol{\theta}\right)^{\textrm{T}}\left(\mathbf{b}-\mathbf{D}\boldsymbol{\theta}\right)+\mathbf{\boldsymbol{\theta}}^{\textrm{T}}\mathbf{P}^{-1}\mathbf{\boldsymbol{\theta}}\right\}  \right]\,d\boldsymbol{\theta}
$$
&lt;/p&gt;

&lt;div style=&quot;background-color: #FFF8C6; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px;&quot;&gt;

&lt;p align=&quot;justify&quot;&gt;Two very important tricks required to perform the above integral are:

&lt;ol&gt;
  &lt;li&gt;&lt;b&gt;Completing the square&lt;/b&gt;&lt;/li&gt;
  $$x^{\textrm{T}}Ax+x^{\textrm{T}}b+c=\left(x-h\right)^{\textrm{T}}A\left(x-h\right)+k$$
  where 
  $$h=-\dfrac{1}{2}A^{-1}b\;\;\;k=c-\dfrac{1}{4}b^{\textrm{T}}A^{-1}b$$
  &lt;li&gt;&lt;b&gt;Multi-dimensional Gaussian Integral&lt;/b&gt;&lt;/li&gt;
  $$
  \int\textrm{exp}\left[-\dfrac{1}{2}x^{\textrm{T}}Ax\right]dx=\sqrt{\left|2\pi A^{-1}\right|}
  $$
&lt;/ol&gt;

&lt;/p&gt;
&lt;/div&gt;

&lt;p align=&quot;justify&quot;&gt;Using the above two important tricks, the Bayesian Evidence is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{Z}=\left({\prod_{i}}\sqrt{2\pi\sigma_{i}^{2}}\right)^{-1}\textrm{exp}\left[-\dfrac{1}{2}\left(\mathbf{k}+\mathbf{b}^{\textrm{T}}\mathbf{b}\right)\right]\,\sqrt{\dfrac{\left|2\pi\left(\mathbf{D}^{\textrm{T}}\mathbf{D}+\mathbf{P}^{-1}\right)^{-1}\right|}{\left|2\pi\mathbf{P}^{-1}\right|}}&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;where $\mathbf{k}=-\left(\mathbf{D}^{\textrm{T}}\mathbf{b}\right)^{\textrm{T}}\left(\mathbf{D}^{\textrm{T}}\mathbf{D}+\mathbf{P}^{-1}\right)^{-1}\left(\mathbf{D}^{\textrm{T}}\mathbf{b}\right)
$

Using the above results, the log-Bayesian Evidence for the two models, $\mathcal{M}_{1}$ and $\mathcal{M}_{2}$ are 238.458 and 243.338 respectively. Therefore, the log-Bayes Factor, $\textrm{log B}_{21}$, which is simply the difference between the two numbers is 4.88, thus showing that  $\mathcal{M}_{2}$ is strongly favoured model over $\mathcal{M}_{1}$ (refer to the &lt;a href=&quot;https://harry45.github.io/blog/2017/05/Bayesian-Model-Selection&quot;&gt;Jeffreys’ scale&lt;/a&gt; in our previous blog post). 
&lt;/p&gt;

&lt;h2&gt;SDDR - Savage-Dickey Density Ratio&lt;/h2&gt;

&lt;p align=&quot;justify&quot;&gt;Savage-Dickey Density Ratio (SDDR) is a useful method for comparing nested models and is in fact, a good approximation to the Bayes Factor. For some parameter values, the extended model $\mathcal{M}_{1}$ reduces to the simpler model $\mathcal{M}_{0}$. Consider a complex model made up of two sets of parameters $\boldsymbol{\theta}=\left(\boldsymbol{\psi},\,\boldsymbol{\phi}\right)$ and the simple model is obtained by setting $\boldsymbol{\phi}=\boldsymbol{\phi}_{0}$. If we further assume that the priors are separable, which is a common scenario: 

$$
\mathcal{P}\left(\boldsymbol{\psi},\,\boldsymbol{\phi}\left|\mathcal{M}_{1}\right.\right)=\mathcal{P}\left(\boldsymbol{\psi}\left|\mathcal{M}_{1}\right.\right)\mathcal{P}\left(\boldsymbol{\phi}\left|\mathcal{M}_{1}\right.\right)
$$

then, the Bayes Factor can, in principle, be written as 

\begin{equation}
B_{01}=\left.\dfrac{\mathcal{P}\left(\boldsymbol{\phi}\left|\mathcal{D},\,\mathcal{M}_{1}\right.\right)}{\mathcal{P}\left(\boldsymbol{\phi}\left|\mathcal{M}_{1}\right.\right)}\right|_{\boldsymbol{\phi}=\boldsymbol{\phi}_{0}}
\end{equation}

The above equation is referred to as the Savage-Dickey Density Ratio. In short, the SDDR is simply the ratio of the marginal posterior to the prior of the complex model, evaluated at the nested point, that is, at the simpler model's parameters value. 

The SDDR is important to judge whether the inclusion of extra parameters in nested models are important when we are trying to fit the observed data. Moreover, in most problems, it is computationally challenging to compute the Bayesian Evidence and hence the Bayes Factor, as it is a multi-dimensional integral. The SDDR provides an alternative avenue to compute the Bayes Factor without computing the Bayesian Evidence provided the models are nested.
&lt;/p&gt;

</description>
        <pubDate>Sun, 21 May 2017 07:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2017/05/Bayesian-Evidence-The-Gaussian-Linear-Model</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2017/05/Bayesian-Evidence-The-Gaussian-Linear-Model</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>Bayesian Model Selection</title>
        <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
  MathJax.Hub.Config({ TeX: { extensions: [&quot;color.js&quot;] }});
&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;Given a data set, one could presumably use various models to explain the data. One of the key questions underlying science is that of model selection: how do we select between competing theories which purport to explain observed data? The great paradigm shifts in science fall squarely into this domain. With so many models available to us, overfitting remains a real issue. Hence, choosing the most suitable model is essential in the Statistics world.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;In the context of astronomy - as with most areas of science - the next two decades will see a massive increase in data volume through large surveys such as the Square Kilometre Array (SKA) and the Large Synoptic Survey Telescope (LSST). Robust statistical analysis to perform model selection at scale will be a critical factor in the success of such future surveys.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Although, we might have very good data, we may not know when to stop fitting. Two competing models may equally fit the data properly but how do we choose the most appropriate model? The solution in fact lies in the simpler model being preferred. This is known as the &lt;i&gt;Occam’s razor&lt;/i&gt;. The complex model which explains the data somewhat better compared to the simpler model should be penalised for the additional parameters that it introduces. Extra parameters bring about lack of predictability. In this spectrum, Bayesian model selection is becoming an increasingly important tool to determine whether or not the introduction of a new parameter is justified by the data.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;In this post, we will focus only on the calculation of the &lt;b&gt;Bayesian Evidence&lt;/b&gt;, which is often regarded as the average likelihood under the prior. The evidence is the normalisation integral of the product of the likelihood and the prior and is often referred to as the &lt;i&gt;model likelihood&lt;/i&gt;, or the &lt;i&gt;marginal likelihood&lt;/i&gt; and is denoted by $\mathbb{Z}$. It is interpreted as the probability of the data $\left(\mathcal{D}\right)$ given the model $\left(\mathcal{M}\right)$. The Bayesian Evidence is given as &lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbb{Z}\equiv\mathcal{P}\left(\mathcal{D}\left|\mathcal{M}\right.\right)=\int\mathcal{P}\left(\mathcal{D}\left|\boldsymbol{\theta},\,\mathcal{M}\right.\right)\,\mathcal{P}\left(\boldsymbol{\theta}\left|\mathcal{M}\right.\right)\,d\boldsymbol{\theta}
\end{equation}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;where $\mathcal{P}\left(\mathcal{D}\left|\boldsymbol{\theta},\,\mathcal{M}\right.\right)$ is the likelihood function which should typically reflect the way the data is obtained and $\mathcal{P}\left(\boldsymbol{\theta}\left|\mathcal{M}\right.\right)$ is the prior distribution for the parameters. Using Bayes' theorem, we can write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{P}\left(\mathcal{M}\left|\mathcal{D}\right.\right)=\dfrac{\mathcal{P}\left(\mathcal{D}\left|\mathcal{M}\right.\right)\mathcal{P}\left(\mathcal{M}\right)}{\mathcal{P}\left(\mathcal{D}\right)}&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;The left-hand side of equation is the posterior probability of the model given the data. Consider two competing models $\mathcal{M}_{1}$ and $\mathcal{M}_{2}$. The ratio of the respective posterior probabilities of the models, given the data is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\mathcal{P}\left(\mathcal{M}_{1}\left|\mathcal{D}\right.\right)}{\mathcal{P}\left(\mathcal{M}_{2}\left|\mathcal{D}\right.\right)}=\dfrac{\mathcal{P}\left(\mathcal{D}\left|\mathcal{M}_{1}\right.\right)}{\mathcal{P}\left(\mathcal{D}\left|\mathcal{M}_{2}\right.\right)}\,\dfrac{\mathcal{P}\left(\mathcal{M}_{1}\right)}{\mathcal{P}\left(\mathcal{M}_{2}\right)}&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;Note that $\mathcal{P}\left(\mathcal{D}\right)$ is only a constant and can be dropped when calculating the ratio of posterior probability of the models. The ratio $$B_{12}=\dfrac{\mathcal{P}\left(\mathcal{D}\left|\mathcal{M}_{1}\right.\right)}{\mathcal{P}\left(\mathcal{D}\left|\mathcal{M}_{2}\right.\right)}$$ is the &lt;b&gt;Bayes factor&lt;/b&gt; and is, in fact, the ratio of the models' evidences. The ratio $$\dfrac{\mathcal{P}\left(\mathcal{M}_{1}\left|\mathcal{D}\right.\right)}{\mathcal{P}\left(\mathcal{M}_{2}\left|\mathcal{D}\right.\right)}$$ is the posterior odds while $$\dfrac{\mathcal{P}\left(\mathcal{M}_{1}\right)}{\mathcal{P}\left(\mathcal{M}_{2}\right)}$$ is the prior odds. Therefore, in words, one can describe the above as &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{posterior odds}=\textrm{Bayes Factor}\times\textrm{prior odds}&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;In the case of non-committal priors on the models, that is, $\mathcal{P}\left(\mathcal{M}_{1}\right)=\mathcal{P}\left(\mathcal{M}_{2}\right)$, the posterior odds are just equal to the Bayes factor. As $B_{12}$ increases, our belief that model $\mathcal{M}_{1}$ is better than model $\mathcal{M}_{2}$ increases. Otherwise, model $\mathcal{M}_{2}$ will be the preferred one. Therefore, the Bayes factor indicates how the relative odds vary in the light of new data, irrespective of the prior odds of the models. The Bayes factor is typically interpreted by referring to the Jeffreys' scale (see &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572&quot;&gt;Kass et al., 1995&lt;/a&gt;). It is an empirically determined scale as shown in the table below &lt;/p&gt;

&lt;style&gt;
table, td, th {
    border: 1px solid black;
}

table {
    border-collapse: collapse;
    width: 53%;
}

&lt;/style&gt;

&lt;style type=&quot;text/css&quot;&gt;
	table.tableizer-table {
		font-size: 14px;
		border: 2px solid #CCC; 
		font-family: Arial, Helvetica, sans-serif;
	} 
	.tableizer-table td {
		padding: 6px;
		margin: 3px;
		border: 1px solid #CCC;
	}
	.tableizer-table th {
		background-color: #104E8B;
		padding: 6px; 
		color: #FFF;
		font-weight: bold;
	}
&lt;/style&gt;

&lt;table class=&quot;tableizer-table&quot; align=&quot;center&quot;&gt;
&lt;thead&gt;&lt;tr class=&quot;tableizer-firstrow&quot;&gt;&lt;th&gt;$\textrm{ln }\left(B_{12}\right)$&lt;/th&gt;&lt;th&gt;$B_{12}$&lt;/th&gt;&lt;th&gt;Evidence against $\mathcal{M}_{2}$&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;0 to 1&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;1 to 3&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Inconclusive&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;1 to 3&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;3 to 20&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Weak Evidence&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;3 to 5&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;20 to 150&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Moderate Evidence&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&amp;gt;5&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;&amp;gt;150&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Strong Evidence&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

</description>
        <pubDate>Sat, 20 May 2017 07:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2017/05/Bayesian-Model-Selection</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2017/05/Bayesian-Model-Selection</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>Bayesian School 2016</title>
        <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
  MathJax.Hub.Config({ TeX: { extensions: [&quot;color.js&quot;] }});
&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;The 2016 Bayesian School was organised at the University of Stellenbosch from the 21&lt;sup&gt;st&lt;/sup&gt; to 25&lt;sup&gt;th&lt;/sup&gt; November. In particular, the school focused on three main topics namely, Introductory Bayesian Methods, Monte Carlo Methods and Advanced Bayesian Methods which was taught by &lt;a href=&quot;http://astro.ic.ac.uk/aheavens/home&quot;&gt;Prof. Alan Heavens&lt;/a&gt; from Imperial College. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Apart from the lectures, another interesting part of the school was &quot;Research Hacks&quot;. The latter allows participants to meet in small groups, ask questions, exchange ideas and initiate collaboration. While these hacks predominantly followed the said-intentions, this also led to the publication of three papers (see &lt;a href=&quot;https://arxiv.org/abs/1704.03472&quot;&gt;arXiv:1704.03472&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1704.03467&quot;&gt;arXiv:1704.03467&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1704.07830&quot;&gt;arXiv:1704.07830&lt;/a&gt;). &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;I learned quite a lot from Prof. Alan lectures and have also attempted some of the questions he proposed during the school. Also, there were various other interesting lectures, related to Bayesian methods. In particular, I will discuss two topics, namely KL Divergence and Bayesian Hierarchical Modelling below.&lt;/p&gt;

&lt;h2&gt;Kullback-Leibler (KL) Divergence&lt;/h2&gt;
&lt;p align=&quot;justify&quot;&gt;Suppose, we have a prior distribution which we denote as $\pi\left(\boldsymbol{\theta}\right)$ on the set of parameters $\boldsymbol{\theta}$. Afterwards, this is updated via Bayes' Theorem to a posterior distribution which we call $q\left(\boldsymbol{\theta}\right)$ and we want to know how much information we have gained from this update. This information can be quantified using the Kullback-Leibler (KL) Divergence,

\begin{align}
\textrm{D}_{\textrm{KL}}\left(q\left\Vert \pi\right.\right)=\int q\,\textrm{log}\left(\dfrac{q}{\pi}\right)d\theta
\end{align}

It is also referred to as the &lt;i&gt;relative entropy&lt;/i&gt;. If this quantity is measured in base of logarithm of 2, the information gain is in &lt;i&gt;bits&lt;/i&gt;, otherwise if measured in base of logarithm of $e$, the information gain is in &lt;i&gt;nats&lt;/i&gt;. To illustrate the KL Divergence, let us consider a simple example, the KL Divergence of two Gaussians. Consider the prior and posterior distributions with means $\mu_{1},\,\mu_{2}$ and variances $\sigma_{1}^{2},\,\sigma_{2}^{2}$. The KL Divergence of $q$ from $\pi$ is

$$
\textrm{D}\left(q\left\Vert \pi\right.\right)=\dfrac{\left(\mu_{1}-\mu_{2}\right)^{2}}{2\sigma_{1}^{2}}+\dfrac{\sigma_{2}^{2}}{2\sigma_{1}^{2}}-\dfrac{1}{2}-\textrm{log}\left(\dfrac{\sigma_{2}}{\sigma_{1}}\right)
$$

From the above result, if the variance of the posterior distribution, that is, $\sigma_{2}^{2}$ decreases, then the information content increases, while if there is a huge difference between the two means, the information gain may increase significantly.&lt;/p&gt;

&lt;div style=&quot;background-color: #FFF8C6; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px;&quot;&gt;

&lt;p align=&quot;justify&quot;&gt;Another question related to KL Divergence was,&lt;/p&gt;&lt;br /&gt;

&lt;p align=&quot;justify&quot; style=&quot;margin-left: 40px; margin-right: 40px&quot;&gt;&lt;i&gt;We have an experiment where a single datum $x$ is assumed to be drawn from a Gaussian Likelihood of mean $\mu$ and variance $\sigma^{2}$. Compute the KL divergence between an assumed Gaussian prior on $\mu$ (with mean zero and variance $\Sigma$) and the posterior.&lt;/i&gt;&lt;/p&gt;&lt;br /&gt;

For this question, the KL Divergence is given by

$$
\textrm{D}_{\textrm{KL}}\left(q\left\Vert \pi\right.\right)=-\dfrac{1}{2}+\dfrac{\sigma^{2}}{2\left(\Sigma+\sigma^{2}\right)}+\dfrac{x^{2}\Sigma}{2\left(\Sigma+\sigma^{2}\right)^{2}}-\dfrac{1}{2}\textrm{log}\left(\dfrac{\sigma^{2}}{\Sigma+\sigma^{2}}\right)
$$

&lt;p align=&quot;justify&quot;&gt;If we had naively assumed a Dirac-Delta prior on the parameter, that is, $\Sigma=0$, then, $\textrm{D}_{\textrm{KL}}\left(q\left\Vert \pi\right.\right)=0$, which means there is no information gain. From my viewpoint, this example of computing the KL Divergence indicates the essence of incorporating priors in our analysis, which in turn leads us to advocating the Bayesian formalism.&lt;/p&gt;

 
&lt;/div&gt;

&lt;h2&gt;Bayesian Hierarchical Modelling&lt;/h2&gt;

&lt;p align=&quot;justify&quot;&gt;This is also one of the topics which helped me to answer one of my questions: how do we proceed with parameter inference when we have error bars on both dependent and independent variables? The idea behind Bayesian Hierarchical Modelling (BHM) is to split the problem into steps where the full model now consists of a series of sub-models.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;BHM links each sub-models, thereby propagating uncertainties from one sub-model to another. Let us consider a simple example, where we are fitting a straight line, $y=mx$ to a single data point, $X$ and $Y$. The complication is that both $X$ and $Y$ have errors, $\sigma_{X}$ and $\sigma_{Y}$ respectively. The question is: how do we infer $m$? In other words, we want $\mathcal{P}\left(m\left|X,\,Y\right.\right)$.&lt;/p&gt;

&lt;div style=&quot;background-color: #FFF8C6; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px;&quot;&gt;

&lt;p align=&quot;justify&quot;&gt;In this part, we will go through the steps in the BHM. To start with, we first need to write Bayes' Theorem, that is,&lt;/p&gt;

$$
\mathcal{P}\left(m\left|X,\,Y\right.\right)=\dfrac{\mathcal{P}\left(X,\,Y\left|m\right.\right)\mathcal{P}\left(m\right)}{\mathcal{P}\left(X,\,Y\right)}\propto\mathcal{P}\left(X,\,Y\left|m\right.\right)\mathcal{P}\left(m\right)
$$

&lt;p align=&quot;justify&quot;&gt;We then introduce the so-called latent variables, $x$ and $y$ and since we are not interested in them, we will marginalise over them as follows,&lt;/p&gt;


$$
\mathcal{P}\left(m\left|X,\,Y\right.\right)\propto \int {\color{red}\mathcal{P}\left(X,\,Y,\,x,\,y\left|m\right.\right)}\mathcal{P}\left(m\right)dxdy
$$
 
&lt;p align=&quot;justify&quot;&gt;Using the product rule, we have&lt;/p&gt;


$$
\mathcal{P}\left(m\left|X,\,Y\right.\right)\propto\int{\color{red}\mathcal{P}\left(X,\,Y\left|x,\,y,\,m\right.\right){\color{blue}\mathcal{P}\left(x,\,y\left|m\right.\right)}}\mathcal{P}\left(m\right)dxdy
$$
 
&lt;p align=&quot;justify&quot;&gt;In this case, the first probability does not depend on $m$, that is, $\mathcal{P}\left(X,\,Y\left|x,\,y,\,m\right.\right) = \mathcal{P}\left(X,\,Y\left|x,\,y\right.\right)$ and using the product rule again, we have&lt;/p&gt;

$$
\mathcal{P}\left(m\left|X,\,Y\right.\right)\propto\int{\color{red}\mathcal{P}\left(X,\,Y\left|x,\,y\right.\right){\color{blue}{\color{magenta}\mathcal{P}\left(y\left|m,\,x\right.\right)}\mathcal{P}\left(x\left|m\right.\right)}}\mathcal{P}\left(m\right)dxdy
$$
 
&lt;p align=&quot;justify&quot;&gt;Our model is deterministic, that is, $\mathcal{P}\left(y\left|m,\,x\right.\right) = \delta\left(y-mx\right)$. Therefore,&lt;/p&gt;


$$
\mathcal{P}\left(m\left|X,\,Y\right.\right)\propto\int{\color{red}\mathcal{P}\left(X,\,Y\left|x,\,mx\right.\right){\color{magenta}\delta\left(y-mx\right)}{\color{blue}\mathcal{P}\left(x\right)}}\mathcal{P}\left(m\right)dxdy
$$

&lt;p align=&quot;justify&quot;&gt;Assuming uniform priors on $\mathcal{P}\left(x\right)$ and $\mathcal{P}\left(m\right)$, we have, &lt;/p&gt;

$$
\mathcal{P}\left(m\left|X,\,Y\right.\right)\propto\int{\color{red}\mathcal{P}\left(X,\,Y\left|x,\,mx\right.\right)}\,dx
$$

&lt;p align=&quot;justify&quot;&gt;We further assume that we know the sampling distribution of $X$ and $Y$ and that they are indendent such that&lt;/p&gt;

$$
\mathcal{P}\left(X,\,Y\left|x,\,y\right.\right)=\mathcal{P}\left(X\left|x\right.\right)\mathcal{P}\left(Y\left|y\right.\right)
$$
 
&lt;p align=&quot;justify&quot;&gt;We further assume that the errors in $X$ and $Y$ are independent Gaussians and for simplicity we take $\sigma_{X}=\sigma_{Y}=1$. Therefore,&lt;/p&gt;

$$
\mathcal{P}\left(m\left|X,\,Y\right.\right)\propto\int e^{-\frac{1}{2}\left(X-x\right)^{2}}e^{-\frac{1}{2}\left(Y-mx\right)^{2}}dx
$$


&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;After completing the square and performing the above integration, the unnormalised posterior distribution of $m$ is given by, &lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathcal{P}\left(m\left|X,\,Y\right.\right)\propto\dfrac{1}{\sqrt{1+m^{2}}}e^{-\frac{1}{2}\left(\frac{Y-mX}{\sqrt{1+m^{2}}}\right)^{2}}
\label{eq:post_m}
\end{equation}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;One can also consider the joint posterior distribution of $x$ and $m$, which is simply given by Bayes' Theorem,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{P}\left(x,\,m\left|X,\,Y\right.\right)\propto\mathcal{P}\left(X,\,Y\left|x,\,mx\right.\right)\mathcal{P}\left(x\right)\mathcal{P}\left(m\right)&lt;/script&gt;

&lt;p&gt;\begin{equation}
\mathcal{P}\left(x,\,m\left|X,\,Y\right.\right)\propto e^{-\frac{1}{2}\left(X-x\right)^{2}}e^{-\frac{1}{2}\left(Y-mx\right)^{2}}
\end{equation}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Consider the case where $X=10$ and $Y=15$. Using Equation \eqref{eq:post_m}, the posterior distribution of $m$ is shown in the left panel of the figure below. In addition, using Gibbs Sampling, the joint posterior distribution of $x$ and $m$ is shown in the right panel of the figure below. &lt;/p&gt;

&lt;dl class=&quot;wp-caption aligncenter&quot; style=&quot;max-width: 800px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/posterior_m.jpg&quot; alt=&quot;The left panel shows the posterior distribution of $m$ while the right panel shows the joint posterior distribution of $x$ and $m$ using Gibbs Sampling.&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;The left panel shows the posterior distribution of $m$ while the right panel shows the joint posterior distribution of $x$ and $m$ using Gibbs Sampling.&lt;/dd&gt;
&lt;/dl&gt;

</description>
        <pubDate>Thu, 18 May 2017 09:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2017/05/Bayesian-School-2016</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2017/05/Bayesian-School-2016</guid>
        
        
        <category>Workshop and Conference</category>
        
      </item>
    
      <item>
        <title>Linear Regression</title>
        <description>&lt;p align=&quot;justify&quot;&gt;One common problem in Statistics is to learn a functional relationship between independent variables and dependent variable. For example, we may want to know how the price of houses varies with the area of the land, the total size of the house and various other criteria. In this particular case, the price of the houses is the dependent variable, also often referred to as the response variable while the area of the land and total size of the house are the independent variables, also known as the attribute variables.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;We will first begin with linear modelling, that is, given a set of attributes, we want infer a linear relationship between the attributes and the response. The function which we want to fit is typically governed by a set of parameters, say $\theta_{i}$. However, before indulging into this topic, it is worth discussing linear and non-linear models.&lt;/p&gt;

&lt;div style=&quot;background-color: #FFF8C6; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px;&quot;&gt;
&lt;b&gt;Linear and Non-Linear Models&lt;/b&gt;&lt;br /&gt;
The equation

\begin{align}
y=\theta_{0} + \theta_{1}x + \theta_{2}x^2
\end{align}

is a linear model model because it is linear in the parameters $\theta_{i}$. In contrast, the model 

\begin{align}
y=\textrm{sin}\left(\omega x + \phi\right)
\end{align}

is a non-linear model since it includes parameters $\left(\omega,\,\phi\right)$ which are non-linear. 
 
&lt;/div&gt;

&lt;h2&gt;Maximum Likelihood Method&lt;/h2&gt;
&lt;p align=&quot;justify&quot;&gt;Suppose, we now want to fit a polynomial $f\left(x\right)$ of order $M$ to some observed data $\left(x_{i},\,y_{i}\right)$ where $i=0,\,1,\,2,\ldots N-1$. We further assume that they are corrupted with Gaussian noise, $\sigma_{i}$. Then, each observed datum can be described by a Gaussian:

\begin{align}
\mathcal{P}\left(y_{i}\left|\boldsymbol{\theta}\right.\right)=\dfrac{1}{\sqrt{2\pi\sigma_{i}^{2}}}\,\textrm{exp}\left[-\dfrac{1}{2}\left(\dfrac{y_{i}-f\left(x_{i}\left|\boldsymbol{\theta}\right.\right)}{\sigma_{i}}\right)^{2}\right]
\end{align}

where $\boldsymbol{\theta}$ is the set of parameters $\left(\theta_{0},\,\theta_{1},\ldots \theta_{M}\right)$. For simplicity, we will explain this method by using a linear fit to the data, that is, $f\left(x\left|\theta_{0},\,\theta_{1}\right.\right)=\theta_{0}+\theta_{1}x$. Assuming that the data point is independent from each other, the likelihood is simply a product of the individual probability distribution of the above. 

&lt;/p&gt;

&lt;div style=&quot;background-color: #FFF8C6; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px;&quot;&gt;
&lt;b&gt;Matrix and Vector Notations&lt;/b&gt;&lt;br /&gt;
We will define the vector $\mathbf{b}$ as:

$$
\mathbf{b=\left(\begin{array}{c}
\frac{y_{0}}{\sigma_{0}}\\
\\
\vdots\\
\\
\frac{y_{N-1}}{\sigma_{N-1}}
\end{array}\right)}
$$

and the design matrix, $\mathbf{D}$ as: 

$$
\mathbf{D}=\left(\begin{array}{cc}
\frac{1}{\sigma_{0}} &amp;amp; \frac{x_{0}}{\sigma_{0}}\\
\\
\\
\\
\frac{1}{\sigma_{N-1}} &amp;amp; \frac{x_{N-1}}{\sigma_{N-1}}
\end{array}\right)
 
$$
&lt;/div&gt;

&lt;p align=&quot;justify&quot;&gt;Therefore, the likelihood (ignoring the pre-factor) can be written as:

\begin{align}
\mathcal{P}\left(\mathbf{y}\left|\boldsymbol{\theta}\right.\right)\propto\textrm{exp}\left[-\dfrac{1}{2}\left(\mathbf{b-\mathbf{D}\boldsymbol{\theta}}\right)^{\textrm{T}}\left(\mathbf{b-\mathbf{D}\boldsymbol{\theta}}\right)\right]
\label{eq:likelihood}
\end{align}

Our aim is to maximise the likelihood. Therefore, the derivative of the likelihood with respect to the parameters should be equal to zero. However, maximising the likelihood is equivalent to maximising the log-likelihood since the latter is a monotonic transformation applied to the likelihood. Some useful tricks when differentiating with respect to a vector are given below.&lt;/p&gt;

&lt;div style=&quot;background-color: #FFF8C6; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px;&quot;&gt;
&lt;b&gt;Differentiating with respect to a vector&lt;/b&gt;&lt;br /&gt;

$$
\dfrac{\partial\mathbf{A}\mathbf{x}}{\partial\mathbf{x}}=\mathbf{A}
$$

$$
\dfrac{\partial\mathbf{x}^{\textrm{T}}\mathbf{A}}{\partial\mathbf{x}}=\mathbf{A}^{\textrm{T}}
$$

If $\mathbf{A}$ is a symmetric matrix, 
$$
\dfrac{\partial\mathbf{x}^{\textrm{T}}\mathbf{A}\mathbf{x}}{\partial\mathbf{x}}=2\mathbf{x}^{\textrm{T}}\mathbf{A}
$$

For further details, see &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_calculus&quot;&gt;Wikipedia&lt;/a&gt; or the &lt;a href=&quot;http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf&quot;&gt;Matrix Cookbook.&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;justify&quot;&gt;Using Equation \eqref{eq:likelihood} and the above useful identities, &lt;/p&gt;

&lt;p&gt;\begin{align}
\boldsymbol{\theta}_{\textrm{MLE}}=\left(\mathbf{D}^{\textrm{T}}\mathbf{D}\right)^{-1}\mathbf{D}^{\textrm{T}}\mathbf{b}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt; The matrix $\mathbf{C}=\left(\mathbf{D}^{\textrm{T}}\mathbf{D}\right)^{-1}$ is called the covariance matrix and gives the standard uncertainties associated with the parameters determined. In particular, the diagonal elements of this matrix give the variances of the parameters while the off-diagonal elements give the covariances between the parameters $\theta_{j}$ and $\theta_{k}$. Hence, the errors on the parameters are equal to the square root of the diagonal elements of the covariance matrix $\mathbf{C}$.&lt;/p&gt;

&lt;h2&gt;Example - A Physics Problem&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Linear_Regression_Circuit.png&quot; align=&quot;left&quot; width=&quot;420&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;We will use the above explanation to answer a Physics problem (Physics 9702 November 2016 Paper 52). A student is investigating the characteristics of different light-emitting diodes (LEDs). Each LED
needs a minimum potential difference across it to emit light. The circuit is set up as shown on the left. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;The potentiometer is adjusted until the LED just emits light. The potential difference $V$ across the LED is measured. The experiment is repeated for LEDs that emit light of different wavelength $\lambda$. It is suggested that $V$ and $\lambda$ are related by the equation

\begin{align}
V=p\lambda^{q}
\end{align}

where $p$ and $q$ are constants. Therefore, if we are to plot a graph of $\textrm{lg }V$ on the $y$-axis against $\textrm{lg }\lambda$ on the $x$-axis, the gradient of the straight line will correspond to $q$ while the $y$-intercept to $\textrm{lg }p$. To be explicit, 

$$
\textrm{lg }V = q\,\textrm{lg }\lambda + \textrm{lg }p
$$
&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Linear_Regression_Data.png&quot; align=&quot;right&quot; width=&quot;420&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;The values of $V$ and $\lambda$ are given in the table below and we also calculate $\textrm{lg }\lambda$ and $\textrm{lg }V$, with its associated error. The error in $\textrm{lg }V$ is:

$$
\sigma_{\textrm{lg }V} = \dfrac{\Delta V}{V}
$$

See this &lt;a href=&quot;http://phys114115lab.capuphysics.ca/App%20A%20-%20uncertainties/appA%20propLogs.htm&quot;&gt;link&lt;/a&gt; for further detail. For this problem, we calculate $\textrm{lg }\lambda$ and $\textrm{lg }V$ to two decimal places. We assume that each data point is Gaussian distributed with mean, $\mu=\textrm{lg }V$ and standard deviation, $\sigma = \sigma_{\textrm{lg }V}$. An illustration of this assumption is shown on the right. We are now ready to plot the graph of $\textrm{lg }V$ versus $\textrm{lg }\lambda$. We find that the gradient is equal to $2.60$ while the $y$-intercept is equal to $7.56$.&lt;/p&gt;

&lt;style&gt;
table, td, th {
    border: 1px solid black;
}

table {
    border-collapse: collapse;
    width: 53%;
}

&lt;/style&gt;

&lt;style type=&quot;text/css&quot;&gt;
	table.tableizer-table {
		font-size: 13px;
		border: 1px solid #CCC; 
		font-family: Arial, Helvetica, sans-serif;
	} 
	.tableizer-table td {
		padding: 6px;
		margin: 3px;
		border: 1px solid #CCC;
	}
	.tableizer-table th {
		background-color: #104E8B;
		padding: 6px; 
		color: #FFF;
		font-weight: bold;
	}
&lt;/style&gt;

&lt;table class=&quot;tableizer-table&quot; align=&quot;left&quot;&gt;
&lt;thead&gt;&lt;tr class=&quot;tableizer-firstrow&quot;&gt;&lt;th&gt;$\lambda/10^{-9}$ m &lt;/th&gt;&lt;th&gt;$V/\,\textrm{V}$&lt;/th&gt;&lt;th&gt;$\textrm{lg}\left(\lambda/10^{-9}\,\textrm{m}\right)$&lt;/th&gt;&lt;th&gt;$\textrm{lg}\left(V/\,\textrm{V}\right)$&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;630&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$1.9\pm0.1$&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;2.80&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$ 0.28\pm0.05$ &lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;620&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$2.0\pm0.1$&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;2.79&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$0.30\pm0.05$ &lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;590&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$2.3\pm0.1$&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;2.77&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$0.36\pm0.04$&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;520&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$3.1\pm0.1$&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;2.72&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$0.49\pm0.03$&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;490&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$3.7\pm0.1$&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;2.69&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$0.57\pm0.03$&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;470&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$4.1\pm0.1$&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;2.67&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;$0.61\pm0.02$&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p align=&quot;justify&quot; style=&quot;margin-left:32em&quot;&gt;Moreover, the covariance matrix is: 

$$
\mathbf{C}=\left(\begin{array}{cc}
0.670 &amp;amp; -0.258\\
-0.258 &amp;amp; 0.095
\end{array}\right)
 
$$

Hence, 

$$
q=-2.60\pm0.84
$$

$$
\textrm{lg }p = 7.56\pm0.31
$$
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/Linear_Regression_Fit.png&quot; align=&quot;right&quot; width=&quot;420&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot; style=&quot;margin-top:3em&quot;&gt;Therefore, in short, the estimates of $p$ and $q$ are $3.61\times10^{7}$ and $-2.60$ respectively. We can also learn about the correlation between the two parameters from the covariance matrix. In particular, since the off-diagonal elements are negative, this implies that the two parameters are negatively correlated, that is, an increase in one parameter results in the decrease of the other. This is illustrated in the figure below, which also shows the credible intervals at $1\sigma$, $2\sigma$ and $3\sigma$. It is also worth mentioning that the distribution of the two parameters $\left(\textrm{lg }p,\,q\right)$ are Gaussian distributed since we are working with a linear model. In addition to this, we can use the values of $p$ and $q$ to estimate the minimum potential difference required if a different diode is used, for example, a diode emitting a wavelength of $950$ nm. 
&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Linear_Regression_Correlation.png&quot; align=&quot;left&quot; width=&quot;420&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Summary and Conclusion&lt;/h2&gt;
&lt;p align=&quot;justify&quot; style=&quot;margin-left:28em&quot;&gt;In this post, we have gone through one method of inferring parameters, namely, the Maximum Likelihood Estimator. We have also provided an example to illustrate this method. It does provide a good estimate of the parameters along with their associated errors. Moreover, we can also learn about the correlation between the parameters using the covariance matrix. &lt;/p&gt;

&lt;p align=&quot;justify&quot; style=&quot;margin-left:28em&quot;&gt;In addition to this, if we had some prior information on the parameters, then the concept of Bayesian Statistics is invoked. The concept is not very different, except that we would have had a prior probability distribution for each parameter. Additionally, instead of finding the MLE, we will end up having the MAP, that is, the Maximum a Posteriori estimates of the parameters. If uniform distributions are assumed, then the MAP coincide with the MLE.&lt;/p&gt;

</description>
        <pubDate>Thu, 02 Mar 2017 06:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/2017/03/Linear-Regression</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2017/03/Linear-Regression</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>Sampling From Any Distribution</title>
        <description>&lt;p align=&quot;justify&quot;&gt;One of the recent topics which I had to study was how to sample from any distribution. While this seems to be a trivial question, Google did not help me much, although I did also try to post the problem on &lt;a href=&quot;http://stackoverflow.com/questions/40263486/drawing-random-samples-from-any-distribution&quot;&gt;stackoverflow&lt;/a&gt;! Here, we will show three methods which we can use to generate random numbers from a distribution. In particular, we will look at some in-built functions in &lt;code&gt;scipy&lt;/code&gt;, acceptance-rejection sampling and will consider interpolation method as well. The distribution which we will use is given by &lt;/p&gt;

&lt;p&gt;\begin{align}
\mathcal{P}\left(x\right) = \dfrac{k\,x^3}{e^{2x} - 0.1}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;where $k$ is the normalisation constant. Note that we will use the following notations: $\mathcal{P}\left(\centerdot\right)$ is the probability distribution function (PDF) while $\Phi\left(\centerdot\right)$ is the cumulative distribution function (CDF). The generic shape of this distribution follows that of a black body spectrum. This distribution is used only to illustrate the idea of sampling and we do not provide any relevant explanation to the actual physics of black body radiation in this post.&lt;/p&gt;

&lt;h2&gt;Using Scipy&lt;/h2&gt;

&lt;p align=&quot;justify&quot;&gt;The class &lt;code&gt;rv_continuous&lt;/code&gt; in &lt;code&gt;scipy.stats&lt;/code&gt; is straightforward to use. We simply need to define either the PDF or CDF using &lt;code&gt;_pdf&lt;/code&gt; and &lt;code&gt;_cdf&lt;/code&gt; respectively as shown below. We first define the PDF, followed by a function to normalise it. Note also that the PDF that we define in the class should be normalised.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1E4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define function to normalise the PDF&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;normalisation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the distribution using rv_continuous&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;blackbody&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rv_continuous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;const&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p align=&quot;justify&quot;&gt;We are now ready to use our above written functions to find $\mathcal{P}\left(x\right)$, $\Phi\left(x\right)$ and generate samples from the underlying distribution. We first need to instantiate it with the lower and upper limits given by &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; respectively. In principle, if not defined, it will be considered to be from $-\infty$ to $+\infty$.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;blackbody_distribution&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blackbody&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;blackbody_distribution&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Find the normalisation constant first&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;norm_constant&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalisation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# create pdf, cdf, random samples&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blackbody_distribution&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm_constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blackbody_distribution&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm_constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blackbody_distribution&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rvs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm_constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1E4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;dl class=&quot;wp-caption aligncenter&quot; style=&quot;max-width: 700px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/scipy_continuous.jpg&quot; alt=&quot;Samples generated using &amp;lt;code&amp;gt;rv_continuous&amp;lt;/code&amp;gt; from &amp;lt;code&amp;gt;scipy.stats&amp;lt;/code&amp;gt;&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;Samples generated using &lt;code&gt;rv_continuous&lt;/code&gt; from &lt;code&gt;scipy.stats&lt;/code&gt;&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;The above plot shows the PDF, CDF and the samples generated from the distribution. In particular, we choose to draw 10 000 random samples. &lt;code&gt;rv_continuous&lt;/code&gt; becomes useful when one needs more than just the samples. Once we have defined it, we can simply find other properties such as its mean, standard deviation and several more (see the &lt;a href=&quot;https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rv_continuous.html&quot;&gt;documentation&lt;/a&gt; for further details).&lt;/p&gt;

&lt;h2&gt;Acceptance-Rejection Sampling&lt;/h2&gt;

&lt;dl class=&quot;wp-caption alignright&quot; style=&quot;max-width: 265px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/circle_accept_reject.jpg&quot; alt=&quot;Estimating value of $\pi$ using Monte Carlo Method&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;Estimating value of $\pi$ using Monte Carlo Method&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;Imagine we have a square board of size $l$, on which there is a circle of radius $0.5l$ at the centre of the board, that is, at $\left(0.5l,0.5l\right)$ in a Cartesian coordinate system. We are throwing darts randomly on the board, say $N$ times. If $n$ is the number of times that the darts lie within the circle, the probability of throwing the darts successfully into the circle is simply $\frac{n}{N}$. This is also roughly equal to the ratio of the area of the circle to the area of the square board. One could use this idea to have a rough estimate of the value of $\pi$, that is,&lt;/p&gt;
&lt;p&gt;\begin{align}
\pi \approx 4 \times \frac{n}{N}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;As $N$ becomes larger, one would have a more accurate estimate of the number $\pi$. This is the idea behind Monte Carlo sampling. Through this lens, an alternative way to sample from a normalised distribution is to use acceptance-rejection sampling scheme. It is also sometimes referred to as Lahiri's Sampling method. This method is advantageous in the sense that we do not need to know the CDF. However, one downfall is that samples may get rejected very often. Moreover, say we want $N$ random numbers, it is unlikely that we will get that number of samples relatively easily. In this post, we have not implemented this method. In short,&lt;/p&gt;

&lt;p align=&quot;justify&quot; style=&quot;padding: 0px 100px 0px 100px&quot;&gt; suppose $X$ is a scalar random variable taking values in the interval $\left[a, b\right]$ according to the continuous probability density function $f\left(x\right)$. Let $M$ be an upper bound for $f$ on $\left[a, b\right]$, $M$ assumed finite. Choose $x$ uniformly in $\left[a, b\right]$. Then choose $u$ uniformly in $\left[0, M\right]$. If $u\leq f\left(x\right)$, we select $x$. Otherwise we reject $x$ and start over.&lt;/p&gt;

&lt;h2&gt;Interpolation Method&lt;/h2&gt;
&lt;p align=&quot;justify&quot;&gt;What do we do if neither of the two methods work but we do have the PDF? The procedures we adopt here is as follows:&lt;/p&gt;

&lt;ol type=&quot;1&quot;&gt;
  &lt;li&gt;Find the CDF using &lt;code&gt;np.cumsum&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Generate a random number from a uniform distribution, $\mathcal{U}\left[0,1\right]$ &lt;/li&gt;
  &lt;li&gt;Use &lt;code&gt;interp1d&lt;/code&gt; from &lt;code&gt;scipy.interpolate&lt;/code&gt; to estimate the random number.&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# Our Own pdf, cdf and samples&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;own_pdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm_constant&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;own_cdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;own_pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;own_cdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;own_cdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define a function to return N samples&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;genSamples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;func_interp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interp1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;own_cdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;func_interp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;own_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;genSamples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1E4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;dl class=&quot;wp-caption aligncenter&quot; style=&quot;max-width: 700px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/own_cdf_pdf_samples.jpg&quot; alt=&quot;Samples generated using the CDF and interpolation method&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;Samples generated using the CDF and interpolation method&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;Here we have a nice distribution with 10 000 random samples drawn using the CDF. It looks similar to the one using &lt;code&gt;rv_continuous&lt;/code&gt;!&lt;/p&gt;
</description>
        <pubDate>Fri, 28 Oct 2016 09:26:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2016/10/Sampling-From-Any-Distribution</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2016/10/Sampling-From-Any-Distribution</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
  </channel>
</rss>
