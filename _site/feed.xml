<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arrykrishna Mootoovaloo</title>
    <description>501, Lower Main Road, Millskock House, Observatory, Cape Town, 7935</description>
    <link>https://Harry45.github.io/</link>
    <atom:link href="https://Harry45.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 08 Feb 2017 19:41:42 +0400</pubDate>
    <lastBuildDate>Wed, 08 Feb 2017 19:41:42 +0400</lastBuildDate>
    <generator>Jekyll v3.3.0</generator>
    
      <item>
        <title>Sampling From Any Distribution</title>
        <description>&lt;p align=&quot;justify&quot;&gt;One of the recent topics which I had to study was how to sample from any distribution. While this seems to be a trivial question, Google did not help me much, although I did also try to post the problem on &lt;a href=&quot;http://stackoverflow.com/questions/40263486/drawing-random-samples-from-any-distribution&quot;&gt;stackoverflow&lt;/a&gt;! Here, we will show three methods which we can use to generate random numbers from a distribution. In particular, we will look at some in-built functions in &lt;code&gt;scipy&lt;/code&gt;, acceptance-rejection sampling and will consider interpolation method as well. The distribution which we will use is given by &lt;/p&gt;

&lt;p&gt;\begin{align}
\mathcal{P}\left(x\right) = \dfrac{k\,x^3}{e^{2x} - 0.1}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;where $k$ is the normalisation constant. Note that we will use the following notations: $\mathcal{P}\left(\centerdot\right)$ is the probability distribution function (PDF) while $\Phi\left(\centerdot\right)$ is the cumulative distribution function (CDF). The generic shape of this distribution follows that of a black body spectrum. This distribution is used only to illustrate the idea of sampling and we do not provide any relevant explanation to the actual physics of black body radiation in this post.&lt;/p&gt;

&lt;h2&gt;Using Scipy&lt;/h2&gt;

&lt;p align=&quot;justify&quot;&gt;The class &lt;code&gt;rv_continuous&lt;/code&gt; in &lt;code&gt;scipy.stats&lt;/code&gt; is straightforward to use. We simply need to define either the PDF or CDF using &lt;code&gt;_pdf&lt;/code&gt; and &lt;code&gt;_cdf&lt;/code&gt; respectively as shown below. We first define the PDF, followed by a function to normalise it. Note also that the PDF that we define in the class should be normalised.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1E4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define function to normalise the PDF&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;normalisation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the distribution using rv_continuous&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;blackbody&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rv_continuous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;const&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p align=&quot;justify&quot;&gt;We are now ready to use our above written functions to find $\mathcal{P}\left(x\right)$, $\Phi\left(x\right)$ and generate samples from the underlying distribution. We first need to instantiate it with the lower and upper limits given by &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; respectively. In principle, if not defined, it will be considered to be from $-\infty$ to $+\infty$.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;blackbody_distribution&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blackbody&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;blackbody_distribution&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Find the normalisation constant first&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;norm_constant&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalisation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# create pdf, cdf, random samples&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blackbody_distribution&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm_constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blackbody_distribution&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm_constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blackbody_distribution&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rvs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm_constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1E4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;dl class=&quot;wp-caption aligncenter&quot; style=&quot;max-width: 700px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/scipy_continuous.jpg&quot; alt=&quot;Samples generated using &amp;lt;code&amp;gt;rv_continuous&amp;lt;/code&amp;gt; from &amp;lt;code&amp;gt;scipy.stats&amp;lt;/code&amp;gt;&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;Samples generated using &lt;code&gt;rv_continuous&lt;/code&gt; from &lt;code&gt;scipy.stats&lt;/code&gt;&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;The above plot shows the PDF, CDF and the samples generated from the distribution. In particular, we choose to draw 10 000 random samples. &lt;code&gt;rv_continuous&lt;/code&gt; becomes useful when one needs more than just the samples. Once we have defined it, we can simply find other properties such as its mean, standard deviation and several more (see the &lt;a href=&quot;https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rv_continuous.html&quot;&gt;documentation&lt;/a&gt; for further details).&lt;/p&gt;

&lt;h2&gt;Acceptance-Rejection Sampling&lt;/h2&gt;

&lt;dl class=&quot;wp-caption alignright&quot; style=&quot;max-width: 265px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/circle_accept_reject.jpg&quot; alt=&quot;Estimating value of $\pi$ using Monte Carlo Method&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;Estimating value of $\pi$ using Monte Carlo Method&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;Imagine we have a square board of size $l$, on which there is a circle of radius $0.5l$ at the centre of the board, that is, at $\left(0.5l,0.5l\right)$ in a Cartesian coordinate system. We are throwing darts randomly on the board, say $N$ times. If $n$ is the number of times that the darts lie within the circle, the probability of throwing the darts successfully into the circle is simply $\frac{n}{N}$. This is also roughly equal to the ratio of the area of the circle to the area of the square board. One could use this idea to have a rough estimate of the value of $\pi$, that is,&lt;/p&gt;
&lt;p&gt;\begin{align}
\pi \approx 4 \times \frac{n}{N}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;As $N$ becomes larger, one would have a more accurate estimate of the number $\pi$. This is the idea behind Monte Carlo sampling. Through this lens, an alternative way to sample from a normalised distribution is to use acceptance-rejection sampling scheme. It is also sometimes referred to as Lahiri's Sampling method. This method is advantageous in the sense that we do not need to know the CDF. However, one downfall is that samples may get rejected very often. Moreover, say we want $N$ random numbers, it is unlikely that we will get that number of samples relatively easily. In this post, we have not implemented this method. In short,&lt;/p&gt;

&lt;p align=&quot;justify&quot; style=&quot;padding: 0px 100px 0px 100px&quot;&gt; suppose $X$ is a scalar random variable taking values in the interval $\left[a, b\right]$ according to the continuous probability density function $f\left(x\right)$. Let $M$ be an upper bound for $f$ on $\left[a, b\right]$, $M$ assumed finite. Choose $x$ uniformly in $\left[a, b\right]$. Then choose $u$ uniformly in $\left[0, M\right]$. If $u\leq f\left(x\right)$, we select $x$. Otherwise we reject $x$ and start over.&lt;/p&gt;

&lt;h2&gt;Interpolation Method&lt;/h2&gt;
&lt;p align=&quot;justify&quot;&gt;What do we do if neither of the two methods work but we do have the PDF? The procedures we adopt here is as follows:&lt;/p&gt;

&lt;ol type=&quot;1&quot;&gt;
  &lt;li&gt;Find the CDF using &lt;code&gt;np.cumsum&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Generate a random number from a uniform distribution, $\mathcal{U}\left[0,1\right]$ &lt;/li&gt;
  &lt;li&gt;Use &lt;code&gt;interp1d&lt;/code&gt; from &lt;code&gt;scipy.interpolate&lt;/code&gt; to estimate the random number.&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# Our Own pdf, cdf and samples&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;own_pdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm_constant&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;own_cdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;own_pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;own_cdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;own_cdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define a function to return N samples&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;genSamples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;func_interp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interp1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;own_cdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;func_interp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;own_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;genSamples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1E4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;dl class=&quot;wp-caption aligncenter&quot; style=&quot;max-width: 700px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/own_cdf_pdf_samples.jpg&quot; alt=&quot;Samples generated using the CDF and interpolation method&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;Samples generated using the CDF and interpolation method&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;Here we have a nice distribution with 10 000 random samples drawn using the CDF. It looks similar to the one using &lt;code&gt;rv_continuous&lt;/code&gt;!&lt;/p&gt;
</description>
        <pubDate>Fri, 28 Oct 2016 12:26:00 +0400</pubDate>
        <link>https://Harry45.github.io/blog/2016/10/Sampling-From-Any-Distribution</link>
        <guid isPermaLink="true">https://Harry45.github.io/blog/2016/10/Sampling-From-Any-Distribution</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>An Introduction to Gibbs Sampling</title>
        <description>&lt;p align=&quot;justify&quot;&gt;Gibbs sampling is a variant of Markov Chain Monte Carlo (MCMC) method (&lt;a href=&quot;https://en.wikipedia.org/wiki/Gibbs_sampling&quot;&gt;Wikipedia&lt;/a&gt;). The idea is to update one of the parameters at a time, thus requiring conditional distributions. As with the standard Metropolis-Hastings algorithm, samples generated from an accumulated Gibbs chain are correlated with nearby samples and hence, if independent samples are required, it is desired to thin the chain by taking every n&lt;sup&gt;th&lt;/sup&gt; value. Moreover, the initial samples (burn-in) are negelected as they may not actually represent the underlying &quot;true&quot; distribution. The algorithm for Gibbs sampling is as follows:
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Initialise $\boldsymbol{\theta}=\left(\theta_{0},\,\theta_{1},\,\ldots\theta_{n}\right)$.&lt;/li&gt; 
&lt;li&gt;Sample for the parameters as below:&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;Sample $\theta_{0}'$ from $\theta_{0}\left|\theta_{0},\,\theta_{1},\,\ldots\theta_{n}\right.$.&lt;/li&gt;
&lt;li&gt;Sample $\theta_{1}'$ from $\theta_{1}\left|\theta_{0}',\,\theta_{2},\ldots\theta_{n}\right.$.&lt;/li&gt;
&lt;li&gt;$\vdots$&lt;/li&gt;
&lt;li&gt;Sample $\theta_{k}'$ from $\theta_{k}\left|\theta_{0}',\,\theta_{1}',\ldots,\,\theta_{k-1}',\,\theta_{k+1},\,\ldots\theta_{n}\right.$&lt;/li&gt;
&lt;/ul&gt;&lt;/ol&gt;


&lt;p align=&quot;justify&quot;&gt;However, notice that we need the conditional distributions. The advantage of Gibbs sampling is that it reduces the need for &quot;tuning&quot; as in the case for the Metropolis-Hastings algorithm. In particular, the starting point can be guessed or can be found using an optimisation algorithm. &lt;/p&gt;

&lt;h2&gt;Example - Bayesian Linear Regression&lt;/h2&gt;

&lt;p align=&quot;justify&quot;&gt;We consider a simple Bayesian Linear Regression to illustrate Gibbs sampling in practice. Suppose we have the datapoints, $\mathcal{D}=\left\{ x_{i},\,y_{i}\right\} $ for $i=1,\,2,\ldots N$ which have been generated from the model $y=\theta_{0}+\theta_{1}x$.
In other words,&lt;/p&gt; 

\begin{align}
y=\theta_{0}+\theta_{1}x+\epsilon
\end{align}


&lt;p align=&quot;justify&quot;&gt;where $\epsilon\sim\mathcal{N}\left(0.0,\,\sigma_{n}^{2}\right)$. Our aim is the find the full posterior distributions of the parameters $\theta_{0}$ and $\theta_{1}$. The joint posterior distribution is simply &lt;/p&gt;

\begin{align}
\mathcal{P}\left(\theta_{0},\,\theta_{1}\left|\mathcal{D}\right.\right)\propto\mathcal{P}\left(\mathcal{D}\left|\theta_{0},\,\theta_{1}\right.\right)\mathcal{P}\left(\theta_{0},\,\theta_{1}\right)
\end{align}


&lt;p align=&quot;justify&quot;&gt; where we assume factorisable priors, that is, $\mathcal{P}\left(\theta_{0},\,\theta_{1}\right)=\mathcal{P}\left(\theta_{0}\right)\,\mathcal{P}\left(\theta_{1}\right)$ and we consider Gaussian priors such that,&lt;/p&gt;

\begin{align}
\mathcal{P}\left(\theta_{0}\right)\sim\mathcal{N}\left(\mu_{0},\,\Sigma_{0}^{2}\right)\hspace{2cm}\mathcal{P}\left(\theta_{1}\right)\sim\mathcal{N}\left(\mu_{1},\,\Sigma_{1}^{2}\right)
\end{align}

&lt;h2&gt;Procedures&lt;/h2&gt;

&lt;p align=&quot;justify&quot;&gt; We first define the design matrices, $\mathbf{D}_{0}$, $\mathbf{D}_{1}$ and the vector $\mathbf{b}$ as follows:&lt;/p&gt;

$$
\mathbf{D}_{0}=\left[\begin{matrix}
\frac{1}{\sigma_{1}}\cr
\frac{1}{\sigma_{2}}\cr
\vdots\cr
\vdots\cr
\frac{1}{\sigma_{N}}
\end{matrix}\right]\hspace{2cm}\mathbf{D}_{1}=\left[\begin{matrix}
\frac{x_{1}}{\sigma_{1}}\cr
\frac{x_{2}}{\sigma_{2}}\cr
\vdots\cr
\vdots\cr
\frac{x_{N}}{\sigma_{N}}
\end{matrix}\right]\hspace{2cm}\mathbf{b}=\left[\begin{matrix}
\frac{y_{1}}{\sigma_{1}}\cr
\frac{y_{2}}{\sigma_{2}}\cr
\vdots\cr
\vdots\cr
\frac{y_{N}}{\sigma_{N}}
\end{matrix}\right]
$$


&lt;p align=&quot;justify&quot;&gt; As we assume $\sigma_{n}$ to be known, the likelihood can be written as &lt;/p&gt;


\begin{align}
\mathcal{P}\left(\mathcal{D}\left|\theta_{0},\,\theta_{1}\right.\right)\propto\textrm{exp}\left[-\dfrac{1}{2}\left(\mathbf{b}-\theta_{0}\mathbf{D}_{0}-\theta_{1}\mathbf{D}_{1}\right)^{\textrm{T}}\left(\mathbf{b}-\theta_{0}\mathbf{D}_{0}-\theta_{1}\mathbf{D}_{1}\right)\right]
\end{align}


&lt;p align=&quot;justify&quot;&gt; and the prior as &lt;/p&gt;

\begin{align}
\mathcal{P}\left(\theta_{0}\right)\propto\textrm{exp}\left[-\dfrac{1}{2}\left(\dfrac{\theta_{0}^{2}-2\mu_{0}\theta_{0}}{\Sigma_{0}^{2}}\right)\right]\textrm{exp}\left[-\dfrac{1}{2}\left(\dfrac{\theta_{1}^{2}-2\mu_{1}\theta_{1}}{\Sigma_{1}^{2}}\right)\right]
\end{align}


&lt;p align=&quot;justify&quot;&gt; The dependence of $\theta_{0}$ in the log-joint posterior distribution, that is, $\theta_{0}\left|\theta_{1},\,\mathcal{D}\right.$ is simply&lt;/p&gt;

$$
-\dfrac{1}{2}\left[\theta_{0}^{2}\left(\mathbf{D}_{0}^{\textrm{T}}\mathbf{D}_{0}+\dfrac{1}{\Sigma_{0}^{2}}\right)+\theta_{0}\left(2\theta_{1}\mathbf{D}_{0}^{\textrm{T}}\mathbf{D}_{1}-2\mathbf{b}^{\textrm{T}}\mathbf{D}_{0}-\dfrac{2\mu_{0}}{\Sigma_{0}^{2}}\right)\right]
$$


&lt;p align=&quot;justify&quot;&gt; This is simply a quadratic function of $\theta_{0}$. If $a=\mathbf{D}_{0}^{\textrm{T}}\mathbf{D}_{0}+\dfrac{1}{\Sigma_{0}^{2}}$
and $b=2\theta_{1}\mathbf{D}_{0}^{\textrm{T}}\mathbf{D}_{1}-2\mathbf{b}^{\textrm{T}}\mathbf{D}_{0}-\dfrac{2\mu_{0}}{\Sigma_{0}^{2}}$,
then &lt;/p&gt;

$$
\mathcal{P}\left(\theta_{0}\left|\theta_{1},\,\mathcal{D}\right.\right)\propto\textrm{exp}\left[-\dfrac{1}{2}\left(a\theta_{0}^{2}+b\theta_{0}\right)\right]
$$


&lt;p align=&quot;justify&quot;&gt; After completing the square,&lt;/p&gt;

$$
\mathcal{P}\left(\theta_{0}\left|\theta_{1},\,\mathcal{D}\right.\right)\propto\textrm{exp}\left[-\dfrac{a}{2}\left(\theta_{0}+\dfrac{b}{2a}\right)^{2}\right]
$$


&lt;p align=&quot;justify&quot;&gt; This is a Gaussian distribution with mean, $\mu$ and standard deviation, $\sigma$ given by &lt;/p&gt;

$$
\mu=-\dfrac{b}{2a}=\dfrac{-\theta_{1}\Sigma_{0}^{2}\mathbf{D}_{0}^{\textrm{T}}\mathbf{D}_{1}+\Sigma_{0}^{2}\mathbf{b}^{\textrm{T}}\mathbf{D}_{0}+\mu_{0}}{\Sigma_{0}^{2}\mathbf{D}_{0}^{\textrm{T}}\mathbf{D}_{0}+1}
$$


$$
\sigma^{2}=\dfrac{1}{a}=\dfrac{\Sigma_{0}^{2}}{\Sigma_{0}^{2}\mathbf{D}_{0}^{\textrm{T}}\mathbf{D}_{0}+1}
$$


&lt;p align=&quot;justify&quot;&gt; Similarly, it can be shown that the dependence of $\theta_{1}$ in the joint posterior distribution, that is, $\mathcal{P}\left(\theta_{1}\left|\theta_{0},\,\mathcal{D}\right.\right)$ is Gaussian with &lt;/p&gt;

$$
\mu=\dfrac{-\theta_{0}\Sigma_{1}^{2}\mathbf{D}_{1}^{\textrm{T}}\mathbf{D}_{0}+\Sigma_{0}^{2}\mathbf{b}^{\textrm{T}}\mathbf{D}_{1}+\mu_{1}}{\Sigma_{1}^{2}\mathbf{D}_{1}^{\textrm{T}}\mathbf{D}_{1}+1}
$$


$$
\sigma^{2}=\dfrac{\Sigma_{1}^{2}}{\Sigma_{1}^{2}\mathbf{D}_{1}^{\textrm{T}}\mathbf{D}_{1}+1}
$$


&lt;h2&gt;Python Code&lt;/h2&gt;

&lt;p align=&quot;justify&quot;&gt; Now we have all the mathematical tools to write the Python code. The simulated data is available on &lt;a href=&quot;https://github.com/Harry45/Self-Taught/tree/master/Gibbs_Sampling&quot;&gt;Github&lt;/a&gt;. We first define the linear function and the true parameters (for the gradient, $\theta_{1}$ and the y-intercept, $\theta_{0}$) are 2.0 and 0.5. Note that in the code below, we have used m and c for the gradient and the y-intercept. We also the specify the fraction of the chain that we will later consider as burn-in.&lt;/p&gt;



&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# True Parameters&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yint&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Fraction considered as burn-in&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;frac&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Load the data &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loadtxt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data_gibbs.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;


&lt;p align=&quot;justify&quot;&gt;The next step is to create the design matrices $\mathbf{D}_{0}$, $\mathbf{D}_{1}$ and the vector $\mathbf{b}$. We also define the hyper-parameters for the Gaussian priors, followed by defining functions to sample for $m$ and $c$ respectively. We add another function which we will then use to optimize the best-fit parameters. These parameters are then used as the starting point in the Gibbs Sampling scheme.&lt;/p&gt;


&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# Create Design Matrices&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Dm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Dc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define Priors (Gaussian Priors)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mu_m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;si_m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;si_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define Functions for Sampling&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sample_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;si_m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;si_m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sample_yint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;si_c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;si_c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;	
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the log-likelihood for optimisation&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loglikelihood&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; 
  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;chiSquare&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;loglike&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chiSquare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loglike&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Use, for example, Powell method for optimisation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;chi_square&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loglikelihood&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chi_square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Powell'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tol&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1E-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;theta_op&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p align=&quot;justify&quot;&gt;We are now ready to run the sampler. In particular, we fix the number of iterations to 200 000 and we reject the first 20 % of the chains.&lt;/p&gt;


&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2E5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gibbs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_yint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gibbs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frac&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Reject first 20 % of the chains&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;


&lt;p align=&quot;justify&quot;&gt;We finally get our nice 2D posterior distribution of the parameters. &lt;/p&gt;

&lt;dl class=&quot;wp-caption aligncenter&quot; style=&quot;max-width: 500px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/triangle_plot_gibbs.png&quot; alt=&quot;2D posterior plot of the two parameters&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;2D posterior plot of the two parameters&lt;/dd&gt;
&lt;/dl&gt;

&lt;/p&gt;
</description>
        <pubDate>Mon, 17 Oct 2016 16:00:00 +0400</pubDate>
        <link>https://Harry45.github.io/blog/2016/10/Gibbs-Sampling</link>
        <guid isPermaLink="true">https://Harry45.github.io/blog/2016/10/Gibbs-Sampling</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>BIRO - Bayesian Inference for Radio Observations</title>
        <description>&lt;style&gt;
  .bottom-three {
     margin-bottom: 0.5cm;
  }
&lt;/style&gt;

&lt;dl class=&quot;wp-caption alignright&quot; style=&quot;max-width: 420px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/superJEDI.jpg&quot; alt=&quot;superJEDI in 2013 at Flic En Flac&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;superJEDI in 2013 at Flic En Flac&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;The superJEDI was organised in 2013, at Flic En Flac in Mauritius. I was not part of this wonderful event as I was still in my second year of my undergraduate study. However, Sheean and Suraj, who will later become my friends, were part of this amazing JEDI. The interesting fact about JEDI is that it is only in these kinds of meeting that we will come up with brilliant ideas, which can then lead to publications. For me personally, the major reason behind is the active participation of all people, being from undergraduate level to the highest position in the hierarchy.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;During one of those days, as Nadeem and Bruce were walking on the seaside, Bruce proposed the idea of having a Bayesian formalism in the context of radio interferometry. This led to the so-called BIRO (Bayesian Inference for Radio Obervation) project.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;iframe src=&quot;https://player.vimeo.com/video/117391380&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;allowfullscreen&quot;&gt; &lt;/iframe&gt; 
&lt;/div&gt;

&lt;p class=&quot;bottom-three&quot;&gt;

&lt;p align=&quot;justify&quot;&gt;The aim of this project was to determine the scientific parameters, if possible, the systematic parameters also, using the visibility data directly. The conventional way of doing this is to first produce a radio image, from which all science will be done. This seems absurb, isn't it? Why do we have to produce an image when we have the raw data? To some extent, one might argue that the data is dominated by noise and hence it will be hard to work in the Fourier space, that is, with the visibility data. This clearly suggests that the best alternative is to have a full distribution of the parameters of interest, with a summary statistics.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Michelle, who was doing her PhD with Bruce at that time, took hold of the BIRO project. She started working on it as part of her PhD. Iniyan was also part of the BIRO project. While Michelle worked mostly on Bayesian Parameter Estimation, Iniyan's work was on Bayesian Model Selection. In a nutshell, Michelle was able to infer not only the scientific parameters but also the systematic parameters in a fully Bayesian formalism. She used MCMC (Markov Chain Monte Carlo) methods to map the full posterior distributions of the parameters. On the other hand, Iniyan used &lt;a href=&quot;http://johannesbuchner.github.io/PyMultiNest/&quot;&gt; PyMultinest&lt;/a&gt; to calculate the Bayesian Evidence (the quantity which tells us how one model is favoured over another). PyMultinest also returns the posterior distributions of the parameters. An illustration from the work done by Michelle and Iniyan is shown in the above video. Compared to &lt;a href=&quot;http://cdsads.u-strasbg.fr/abs/1974A%26AS...15..417H&quot;&gt;CLEAN&lt;/a&gt;, BIRO is much better in terms of performance.&lt;/p&gt;


&lt;p align=&quot;justify&quot;&gt;Of course, the technique is not without problems. One arguement is that we have to know the sky model first, before proceeding with Bayesian Inference. Therefore, in BIRO projects, we would normally assume a known sky model, which then leads to the second assumption that the positions of the sources are known. Moreover, nested sampling is known to have unreliable performance for higher dimensions. However, the fruitful side of this fantastic idea by Bruce has led to the following publications: &lt;a href=&quot;https://arxiv.org/abs/1501.05304&quot;&gt;BIRO&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1501.07719&quot;&gt;MontBlanc&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1610.03773&quot;&gt;Resolving the blazar CGRaBS J0809+5341&lt;/a&gt;. We are currently extending the BIRO formalism to various other topics in radio astronomy.&lt;/p&gt;

&lt;/p&gt;
</description>
        <pubDate>Thu, 13 Oct 2016 11:00:00 +0400</pubDate>
        <link>https://Harry45.github.io/blog/2016/10/BIRO-Bayesian-Inference-For-Radio-Observations</link>
        <guid isPermaLink="true">https://Harry45.github.io/blog/2016/10/BIRO-Bayesian-Inference-For-Radio-Observations</guid>
        
        
        <category>Research</category>
        
      </item>
    
      <item>
        <title>Surviving A Fast Growing Community Without Dying</title>
        <description>&lt;style&gt;
blockquote {
    display: block;
    margin-top: 1em;
    margin-bottom: 1em;
    margin-left: 100px;
    margin-right: 100px;
}
&lt;/style&gt;

&lt;blockquote&gt;
&lt;p align=&quot;justify&quot;&gt;&lt;i&gt;&quot;Emotions matter. You can tell people how to behave, but you can't tell people how to think and not everyone reacts the same way&quot;&lt;/i&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;dl class=&quot;wp-caption alignright&quot; style=&quot;max-width: 400px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/flavio.jpg&quot; alt=&quot;Flavio giving his talk at PyConZA (2016)&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;Flavio giving his talk at PyConZA (2016)&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;The &lt;a href=&quot;https://za.pycon.org/&quot;&gt;PyConZA&lt;/a&gt; was organised on the 6&lt;sup&gt;th&lt;/sup&gt; and 7&lt;sup&gt;th&lt;/sup&gt; October at the River Club in Observatory. The most impressive talk, at least for me, which had nothing to do with Python was &lt;a href=&quot;https://it.linkedin.com/in/fpercoco&quot;&gt;Flavio&lt;/a&gt;'s talk. It was easy to follow him throughout the talk and he had a real sense of humour. Most importantly, I would strongly recommend this talk (which is now available on &lt;a href=&quot;https://www.youtube.com/watch?v=bW_AEmKbB_o&quot;&gt;Youtube&lt;/a&gt;) as I believe that every young persons willing to join the industry will be well prepared, at least morally.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;He started by defining the 3 most basic terms: system, culture and flexibility which were not even clear to me what they meant until he explained them in simple terms. Flavio defines a system as being a mean of empowering humans to be amazing, a culture as the way humans do things (alternatively another answer from the audience was to define it as the rule which defines how a system works) and flexibility as the level of tolerance for variance in the system. Someone in the audience also defined flexibility as the ability to change the system and the culture! Flavio went on to explain what he really meant by the above three terms. In particular, he emphasized that tolerating variance in your community is a way to empower humans, from any culture which leads them to be simply amazing. On the other hand, community creates processes, which in turn lead to governance. Governance itself is essential to ensure growth. Governance will in principle follow the community and it is therefore important to know and understand our community.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;He further described a set of attitudes and behaviours which are relevant in a growing community. Being a good listener is a crucial factor, although we cannot make everyone happy. Being humble and objective will simplify things to a huge extent. Usually, a community will set the expectations and it is important to have clear expectations in order to be objective. It is also often a good practice to set the bar at a reasonable level. Above all, communicating the expectations is the key and it is sometimes better to over-communicate! Acknowledging our colleagues at the workplace is another important factor. The contribution of each and every person leads to excellent results.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Moreover, unlike computers, humans, from different cultural background, are subject to emotion and different cultures bring different perspectives and hence, it is important that we all strive for diversity. On the other hand, tribal thinking is certainly bad. In fact, it is crucial to build a community of doers, rather than a community of ranters! It is also important to understand that many humans could wear different hats. You could be working for Facebook but you could be holding another research position in academia as well. It happens and therefore it is important to understand others' situations. On this note, we should all be aware of Time Zone! You may be based in Germany but you have a colleague in Japan and hence, you will have to cope with the situation.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;On a conclusive note, Flavio also spoke about statistics. Statistics indicate that statistics is not good! If we give a human a number, he will do anything to make it bigger. As the community grows, the processes will evolve. However, no matter how complicated it gets, &quot;technology is social before it's technical - Gilles Deleuza.&quot; On a final note, saying thanks will always make your colleague feel better and respected. &lt;/p&gt;

</description>
        <pubDate>Mon, 10 Oct 2016 16:00:00 +0400</pubDate>
        <link>https://Harry45.github.io/blog/2016/10/Surviving-Growing-Community</link>
        <guid isPermaLink="true">https://Harry45.github.io/blog/2016/10/Surviving-Growing-Community</guid>
        
        
        <category>Workshop and Conference</category>
        
      </item>
    
      <item>
        <title>JEDI - An Alternate Route to Conventional Workshops</title>
        <description>&lt;p align=&quot;justify&quot;&gt;JEDI (Joint Exchange Development Initiative) is a programme brought forward by &lt;a href=&quot;https://cosmoaims.wordpress.com/2010/01/01/bruce-bassett/&quot;&gt;Prof Bruce Bassett&lt;/a&gt;, Dr Nadeem Oozeer and their team. The idea is straightforward. In most of the conferences, people will normally present their work and if it happens that we go to two different conferences, coincidentally, we might eventually find the same persons giving the same presentations. Therefore, instead of spending so much money on conferences, why don't we bring all academic staffs and students together for one week and work on a completely new project? &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;So, we would normally organise it in a beautiful place (possibly in a different country, so that, some of the local students can benefit from it), work together, cook together, stay together, thus eliminating the gap between students and academic staffs. Having done so in the past few years, several projects initiated in JEDIs, have not only led to publications (see for example &lt;a href=&quot;https://arxiv.org/abs/1501.05304&quot;&gt;BIRO&lt;/a&gt;) but also, students have had the chance to continue further studies, for example, undergraduate to MSc, MSc to PhD and so forth. In short, students have been able to advance in their career. For me personally, this programme also gives the students the opportunity to be future leaders.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;My participations in JEDIs have enabled me to learn a lot. I was lucky that I got the chance to be part of it at a very young age. I had good grades for my undergraduate studies (BSc (Hons) Physics with Computing) in Mauritius but I had no idea of where to use the skills and knowledge I had gained through this degree. It was not only until I participated in one JEDI that I came to learn about Data Science, Machine Learning, Deep Learning and so forth. The transfer of scientific ideas, skills, knowledge towards solving various other non-scientific problems is simply amazing. It all came back and I could use my mathematics and coding techniques again. As Steve Jobs rightly said, &quot;You can't connect the dots looking forward; you can only connect them looking backwards. So you have to trust that the dots will somehow connect in your future.&quot;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Now that many students have an experience of how the JEDI works, we, students work together with Prof Bruce Bassett and his team to organise various such events in different places/countries in Africa. Until now, I have participated in a few of them and I was also sometimes on the organising team.&lt;/p&gt;
</description>
        <pubDate>Sun, 09 Oct 2016 12:00:00 +0400</pubDate>
        <link>https://Harry45.github.io/blog/2016/10/JEDI</link>
        <guid isPermaLink="true">https://Harry45.github.io/blog/2016/10/JEDI</guid>
        
        
        <category>Workshop and Conference</category>
        
      </item>
    
      <item>
        <title>Gaussian Process</title>
        <description>&lt;p align=&quot;justify&quot;&gt;In the most simple term, one can think of Gaussian Process, GP as being distributions over functions. It is a supervised Machine Learning technique and was developed, in the attempt, to solve regression problems (&lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_process&quot;&gt;Wikipedia&lt;/a&gt;). The cool thing about Gaussian Process is that we don't need a &quot;parametric model&quot; to fit the data. It learns using the kernel trick. For an introduction to these techniques, I would recommend the nice review by Prof Zoubin Ghahramani on &lt;a href=&quot;http://www.nature.com/nature/journal/v521/n7553/full/nature14541.html&quot;&gt;Probabilistic Machine Learning and Artificial Intelligence &lt;/a&gt;. Other nice books for GPs are &lt;a href=&quot;http://www.gaussianprocess.org/gpml/&quot;&gt;Gaussian Processes for Machine Learning &lt;/a&gt; by Carl Edward Rasmussen and Christopher K. I. Williams and &lt;a href=&quot;https://mitpress.mit.edu/books/machine-learning-0&quot;&gt;Machine Learning - A Probabilistic Perspective&lt;/a&gt; by Kevin Murphy. Before going into the actual explanation of what a GP is, it is important to have a brief background of how to find the marginals and conditionals of a Multivariate Nomal Distribution. See Chapter 4 from Kevin Murphy's book.&lt;/p&gt;

&lt;h2&gt;Marginals and Conditionals&lt;/h2&gt;

&lt;p align=&quot;justify&quot;&gt; Consider a 2D Gaussian Distribution whose parameters are given by &lt;/p&gt;

&lt;p&gt;\begin{align}
\boldsymbol{\mu}=\left(\begin{matrix}
\mu_{1} \cr
\mu_{2}
\end{matrix}\right)\hspace{3cm}\boldsymbol{\Sigma}=\left(\begin{matrix}
\Sigma_{11} &amp;amp; \Sigma_{12}\cr
\Sigma_{21} &amp;amp; \Sigma_{22}
\end{matrix}\right)
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Then, the joint distribution of $x_{1}$ and $x_{2}$, that is, $\mathcal{P}\left(x_{1},\,x_{2}\right)$ can be written as&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathcal{P}\left(x_{1},\,x_{2}\right)=\dfrac{1}{\left|2\pi\boldsymbol{\Sigma}\right|^{\frac{1}{2}}}\,\textrm{exp}\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^{\textrm{T}}\boldsymbol{\Sigma}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}\right)\right]
\label{eq:2d_gaussian}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Using $\mathcal{P}\left(x_{1},\,x_{2}\right)=\mathcal{P}\left(x_{1}\left|x_{2}\right.\right)\mathcal{P}\left(x_{2}\right)$, it can be shown that the conditional $\mathcal{P}\left(x_{1}\left|x_{2}\right.\right)$ is also a quadratic with the mean and variance given by &lt;/p&gt;

&lt;p&gt;\begin{align}
\mu_{1\left|2\right.}=\mu_{1}+\Sigma_{12}\Sigma_{22}^{-1}\left(x_{2}-\mu_{2}\right)\hspace{3cm}\Sigma_{1\left|2\right.}=\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
\label{eq:marginals}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;such that&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathcal{P}\left(x_{1}\left|x_{2}\right.\right)=\dfrac{1}{\sqrt{2\pi\Sigma_{1\left|2\right.}}}\,\textrm{exp}\left[-\dfrac{1}{2}\left(\dfrac{x_{1}-\mu_{1\left|2\right.}}{\Sigma_{1\left|2\right.}}\right)^{2}\right]
 \label{eq:marginals_distribution}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Intuitively, this can be understood by the fact that if we take a slice across a 2D Multivariate Normal Distribution as shown in the picture below, the resulting distribution is also a Gaussian Distribution. &lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/images/2D-Gaussian.png&quot; alt=&quot;2D Gaussian Distribution&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;GP for Regression&lt;/h2&gt;

&lt;p align=&quot;justify&quot;&gt;In Bayesian Parameter Inference, in which case we have a parametric model $\mathcal{M}$ and some data $\mathcal{D}$, we would want to infer the posterior distribution of the parameters, that is, $\mathcal{P}\left(\boldsymbol{\theta}\left|\mathcal{M},\,\mathcal{D}\right.\right)$. On the other hand, a GP in fact defines a prior over the functions itself, such that it can be mapped to a posterior given some data $\mathcal{D}$. In this section, we will show how we can take adavantage of the nice properties of the above to predict the likely value of a function for a given $x_{*}$. The major assumption we will make is that the testing set of data is from the same distribution as the training set of data.&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Noise-Free Case&lt;/b&gt;&lt;/h4&gt;

&lt;p align=&quot;justify&quot;&gt;Suppose we have a set of data, $\mathcal{D}=\left\{ \left(x_{i},\,f_{i}\right)\right\}$ for $i=1,2,3,\ldots N$, where $f$ is assumed to be an observable without noise. Now, given $x_{*}$, we would like to predict $f_{*}$ as well as its variance, $\Sigma_{*}$. In a sense, we effectively want the posterior distributions of $f_{*}$ given $x_{*}$, $x$ and $f$. Using Bayes' Theorem, we can write &lt;/p&gt;

&lt;p&gt;\begin{align}
\mathcal{P}\left(\mathbf{f}\left|\mathbf{y},\,\mathbf{X}\right.\right)=\dfrac{\mathcal{P}\left(\mathbf{y}\left|\mathbf{X},\,\mathbf{f}\right.\right)\mathcal{P}\left(\mathbf{f}\left|\mathbf{X}\right.\right)}{\mathcal{P}\left(\mathbf{y}\left|\mathbf{X}\right.\right)}
\label{eq:BayesTheorem}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;where $\mathcal{P}\left(\mathbf{f}\left|\mathbf{y},\,\mathbf{X}\right.\right)$ is the posterior distribution of the function, $\mathcal{P}\left(\mathbf{f}\left|\mathbf{X}\right.\right)$ is the prior, $\mathcal{P}\left(\mathbf{y}\left|\mathbf{X},\,\mathbf{f}\right.\right)$ is the likelihood and $\mathcal{P}\left(\mathbf{y}\left|\mathbf{X}\right.\right)$ is the marginal likelihood which is simply given by&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathcal{P}\left(\mathbf{y}\left|\mathbf{X}\right.\right)=\int\mathcal{P}\left(\mathbf{y}\left|\mathbf{X},\,\mathbf{f}\right.\right)\mathcal{P}\left(\mathbf{f}\left|\mathbf{X}\right.\right)d\mathbf{f}
\label{eq:marginal_likelihood}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Given now a new data $x_{*}$, the posterior distribution of $f_{*}$ is simply obtained by marginalisation.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{P}\left(y_{*}\left|\mathbf{y},\,\mathbf{X},\,x_{*}\right.\right)=\int\mathcal{P}\left(y_{*}\left|\mathbf{f},\,x_{*}\right.\right)\mathcal{P}\left(\mathbf{f}\left|\mathbf{y},\,\mathbf{X}\right.\right)\,d\mathbf{f}&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;The prior on the regression function which is a Gaussian Process, GP denoted by &lt;/p&gt;

&lt;p&gt;\begin{align}
f\left(\mathbf{X}\right)\sim\textrm{GP}\left(\boldsymbol{\mu}\left(\mathbf{X}\right),\,\kappa\left(\mathbf{X},\,\mathbf{X}\right)\right)
\label{definition_gp}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;where $\boldsymbol{\mu}\left(\mathbf{X}\right)$ is the mean of the function while $\kappa\left(\mathbf{X},\,\mathbf{X}'\right)$ is the covariance matrix, which is constructed using a kernel. In particular, for this example, we will consider the squared-exponential kernel,&lt;/p&gt;

&lt;p&gt;\begin{align}
\kappa\left(x,\,x\right)=\sigma^{2}\,\textrm{exp}\left(-\dfrac{\left(x-x\right)^{2}}{2\ell^{2}}\right)
\label{squared_exponential}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;where $\ell$ and $\sigma$ control the horizontal and vertical variations respectively. The kernel encapsulates the correlation between two datapoints. What we would ideally want is the following - if two datapoints are close to each other, that is, $x-x'\approx0$, we would expect strong correlation, while if $x-x'\rightarrow\infty$, there should be minumum correlation between $x$ and $x'$. The kernel trick allows us to easily implement this. Note that there exists various other kernel types (for more details refer to &lt;a href=&quot;http://www.gaussianprocess.org/gpml/&quot;&gt;Gaussian Processes for Machine Learning &lt;/a&gt; by Carl Edward Rasmussen and Christopher K. I. Williams)&lt;/p&gt;

&lt;p&gt;\begin{align}
\kappa\left(x,\,x\right)=\begin{cases}
\begin{matrix}
\sigma^{2}\cr
0
\end{matrix} &amp;amp; \begin{matrix}
x-x=0\cr
x-x\rightarrow\infty
\end{matrix}\end{cases}
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;For the regression problem, the joint distribution is given by &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left(\begin{matrix}
\mathbf{f}\cr
\mathbf{f}_{*}
\end{matrix}\right)\sim\left(\left(\begin{matrix}
\boldsymbol{\mu}\cr
\boldsymbol{\mu}_{*}
\end{matrix}\right),\,\left(\begin{matrix}
\mathbf{K} &amp; \mathbf{K}_{*}\cr
\mathbf{K}_{*}^{\textrm{T}} &amp; \mathbf{K}_{**}
\end{matrix}\right)\right) %]]&gt;&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;where $\mathbf{K}_{**} = \kappa\left(\mathbf{X}_{*},\,\mathbf{X}_{*}\right)$. Using the above results given by Equations \eqref{eq:marginals}, the posterior distribution $\mathcal{P}\left(f_{*}\left|\mathbf{y},\,\mathbf{X},\,x_{*}\right.\right)$ is simply&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{P}\left(\mathbf{f}\left|\mathbf{X}_{*},\,\mathbf{X},\,\mathbf{f}\right.\right)=\mathcal{N}\left(\mathbf{f}_{*}\left|\boldsymbol{\mu}_{*},\,\boldsymbol{\Sigma}_{*}\right.\right)&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt; where &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\mu}_{*}=\boldsymbol{\mu}\left(\mathbf{X}_{*}\right)+\mathbf{K}_{*}^{\textrm{T}}\mathbf{K}^{-1}\left(\mathbf{f}-\boldsymbol{\mu}\left(\mathbf{X}\right)\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\Sigma}_{*}=\mathbf{K}_{**}-\mathbf{K}_{*}^{\textrm{T}}\mathbf{K}^{-1}\mathbf{K}_{*}&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;We would notionally assume a mean function equal to 0 and the kernel must be &lt;a href=&quot;https://en.wikipedia.org/wiki/Positive-definite_matrix&quot;&gt;positive definite&lt;/a&gt;. Hence, &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\mu}_{*}=\mathbf{K}_{*}^{\textrm{T}}\mathbf{K}^{-1}\mathbf{f}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\Sigma}_{*}=\mathbf{K}_{**}-\mathbf{K}_{*}^{\textrm{T}}\mathbf{K}^{-1}\mathbf{K}_{*}&lt;/script&gt;

&lt;h4&gt;&lt;b&gt;Noisy Case&lt;/b&gt;&lt;/h4&gt;

&lt;p&gt;If instead, we were to observe a noisy function given by $y=f\left(x\right)+\epsilon$ where $\epsilon\sim\mathcal{N}\left(0,\,\sigma_n^2\right)$, the matrix $\mathbf{K}$ is simply given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{K}_n=\mathbf{K}+\sigma_n^2\mathbf{I}&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;assuming that the observed data is corrupted by the noise independently. Then the prediction for the new function along with its uncertainty is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\mu}_{*}=\mathbf{K}_{*}^{\textrm{T}}\mathbf{K}_{n}^{-1}\mathbf{f}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\Sigma}_{*}=\mathbf{K}_{**}-\mathbf{K}_{*}^{\textrm{T}}\mathbf{K}_{n}^{-1}\mathbf{K}_{*}&lt;/script&gt;

&lt;h2&gt;Learning the Kernel Parameters&lt;/h2&gt;
&lt;p align=&quot;justify&quot;&gt; The way to estimate the parameters is via maxmising the marginal likelihood, $\mathcal{P}\left(\mathbf{y}\left|\mathbf{X}\right.\right)$ which is simply a multivariate Gaussian distribution given by $\mathcal{N}\left(\mathbf{y}\left|0,\,\mathbf{K}_{n}\right.\right)$. Hence, &lt;/p&gt;

&lt;p&gt;\begin{align}
\textrm{log }\mathcal{P}\left(\mathbf{y}\left|\mathbf{X}\right.\right)=-\dfrac{1}{2}\left[\mathbf{y}^{\textrm{T}}\mathbf{K}_{n}^{-1}\mathbf{y}+\textrm{log}\left|\mathbf{K}_n \right|+N\textrm{log}\left(2\pi\right)\right]
\end{align}&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;where $N$ is the number of test data points. Essentially, the third term is does not depend on the kernel parameters and is simply an additive constant. Hence, the derivative of the marginal likelihood&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial}{\partial\theta_{j}}\textrm{log }\mathcal{P}\left(\mathbf{y}\left|\mathbf{X}\right.\right)=-\dfrac{1}{2}\left[\mathbf{y}^{\textrm{T}}\dfrac{\partial\mathbf{K}_{n}^{-1}}{\partial\theta_{j}}\mathbf{y}+\dfrac{\partial}{\partial\theta_{j}}\textrm{log}\left|\mathbf{K}_{n}\right|\right]&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;can further be simplified using the fact that $\dfrac{\partial\mathbf{K}_{n}^{-1}}{\partial\theta_{j}}=-\mathbf{K}_{n}^{-1}\dfrac{\partial\mathbf{K}_{n}}{\partial\theta_{j}}\mathbf{K}_{n}^{-1}$ and that $ \dfrac{\partial}{\partial\theta_{j}}\textrm{log}\left|\mathbf{K}_{n}\right|=\textrm{tr}\left(\mathbf{K}_{n}^{-1}\dfrac{\partial\mathbf{K}_{n}}{\partial\theta_{j}}\right)$ such that &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial}{\partial\theta_{j}}\textrm{log }\mathcal{P}\left(\mathbf{y}\left|\mathbf{X}\right.\right)=\dfrac{1}{2}\left[\mathbf{y}^{\textrm{T}}\mathbf{K}_{n}^{-1}\dfrac{\partial\mathbf{K}_{n}}{\partial\theta_{j}}\mathbf{K}_{n}^{-1}\mathbf{y}-\textrm{tr}\left(\mathbf{K}_{n}^{-1}\dfrac{\partial\mathbf{K}_{n}}{\partial\theta_{j}}\right)\right]&lt;/script&gt;

&lt;p align=&quot;justify&quot;&gt;Given that we now have the gradient, one could easily use an optimisation algorithm to estimate the hyper-parameters. An alternative method is to have a full Bayesian formalism for inferring the posterior distributions of the hyper-parameters.&lt;/p&gt;

&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p align=&quot;justify&quot;&gt;Below, we consider two examples: noise-free and a noisy. For the following, for illustration, we assume that the kernel parameters $\sigma$ and $\ell$ are both equal to 1. We show how the Gaussian Process performs well in the noise-free and equally-spaced data as shown below. We consider a simple sinusoidal function of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f=\textrm{sin}\left(x\right)\hspace{2cm}\left[0,\,2\pi\right]&lt;/script&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/images/example_1_uniform.png&quot; alt=&quot;uniform_gp&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt; On the other hand, we generate noisy data, which are not equally-spaced. The point which we are mostly interested in is that the Gaussian Process basically demonstrates the level of confidence when we have or do not have data. As expected, we would be more confident when we have more data, and less confident when we don't have data. &lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/images/example_1_non_uniform.png&quot; alt=&quot;non_uniform_gp&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 08 Oct 2016 16:00:00 +0400</pubDate>
        <link>https://Harry45.github.io/blog/2016/10/Gaussian-Process</link>
        <guid isPermaLink="true">https://Harry45.github.io/blog/2016/10/Gaussian-Process</guid>
        
        
        <category>Machine Learning and Statistics</category>
        
      </item>
    
      <item>
        <title>A Brief Overview of my Undergraduate Project</title>
        <description>&lt;dl class=&quot;wp-caption alignright&quot; style=&quot;max-width: 350px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/overlay_1.jpg&quot; alt=&quot;Hydra A&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;Hydra A&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt; X-ray cavities are bubbles which have been formed by the central active galactic nucleus (AGN) found at the centre of galaxy clusters, groups and giant ellipticals. There is an increasing evidence, as observed from radio/x-ray overlays, that these cavities are formed by the tip of jets of the central radio galaxy and is connected back to the nucleus, in a channel-like structure. These cavities are detected in x-ray images as surface brightness depressions. Besides, most these cavities are filled with radio-emission at 1.4 GHz while, there do exist cavities, referred to as ghost bubbles which are misaligned with respect to the jet and are observed at lower radio frequency.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt; The formation of X-ray cavities is a consequence of AGN feedback which is responsible for the formation of other structures such as shocks and ripples. Typically, the formation of these structures may offset cooling in galaxy clusters and might presumably be one of the solutions to the classical cooling flow issue, which was proposed by &lt;a href=&quot;https://ned.ipac.caltech.edu/level5/Fabian3/frames.html&quot;&gt;Fabian (1994)&lt;/a&gt;. &lt;/p&gt;

&lt;dl class=&quot;wp-caption alignleft&quot; style=&quot;max-width: 400px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/overlay_2.jpg&quot; alt=&quot;RBS 797 &quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;RBS 797 &lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt; Originally, it was believed that a cooling flow must be established at the centre of cluster of galaxy as the central atmosphere being very dense and hot, the x-ray gas must lose energy via the emission of x-ray. However, after the launching of Chandra and XMM-Newton satellites in the year 1999, it was observed that relatively little gas actually cools below 2 keV. We have investigated 9 clusters of galaxies, having redshifts $0.01 &amp;lt; z &amp;lt; 0.35$, which host x-ray cavities using CIAO 4.6 and CALDB 4.5.9. These cavities are aged about $10^7$ years. The most commonly used methods for calculating cavity ages include refill timescale $\left(t_r\right)$, buoyancy timescale $\left(t_b\right)$ and sound-crossing timescale $\left(t_s\right)$. In particular, $t_r &amp;lt; t_b &amp;lt; t_s$. The enthalpy of a cavity depends on the nature of lobes and lies between $2pV$ and $4pV$ as described by &lt;a href=&quot;https://arxiv.org/abs/0709.2152&quot;&gt;McNamara and Nulsen (2007)&lt;/a&gt;.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt; The cavity power can be determined using the cavity age and enthalpy and is of the order of $10^{44}\, \textrm{ergs s}^{1}$. In addition, the central radio source of each cluster of galaxy was studied, dealing with both strong and weak radio galaxies. Their radio luminosities at 1.4 GHz were calculated, using symbolic programming method in Matlab 2012Ra and typical value of radio luminosity $\left(\textrm{L}_{\textrm{rad}}\right)$ ranges from $0.9  315 \times 10^{42}\, \textrm{ergs s}^{1}$. Using these values, together with the determined cavity power, a plot of radio luminosity against cavity power was made in the logarithmic scale, and it was found that the cavity power $\textrm{P}_{\textrm{cav}}$ varies as &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{P}_{\textrm{cav}} = \left(5 \times 10^{38\pm1}\right)\,\textrm{L}_{\textrm{rad}}^{0.13\pm0.13}&lt;/script&gt;

&lt;dl class=&quot;wp-caption alignright&quot; style=&quot;max-width: 410px&quot;&gt;

&lt;dt&gt;&lt;a href=&quot;&quot;&gt;&lt;img class=&quot;&quot; src=&quot;/images/overlay_3.jpg&quot; alt=&quot;Abell 2052&quot; /&gt;&lt;/a&gt;&lt;/dt&gt;

&lt;dd&gt;Abell 2052&lt;/dd&gt;
&lt;/dl&gt;

&lt;p align=&quot;justify&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/astro-ph/0605323&quot;&gt;Rafferty et al. (2006)&lt;/a&gt; argued that the formation of x-ray cavities may be modelled by the mass accretion model. In this scenario, the jet is produced when a fraction of the gravitational binding energy of the material accreted is converted into outburst energy. While in another theory developed by &lt;a href=&quot;https://arxiv.org/abs/astro-ph/9810352&quot;&gt;Meier (1999)&lt;/a&gt;, it is possible for cavities to be formed via the spinning of the central rotating black hole. In this model, the spin energy is converted into jet power via a torque applied by the poloidal magnetic field strength.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;There could be different scenarios possible. For example, for Hydra A, the jet is aligned with the cavities while in RBS 797, the jet is roughly perpendicular to the orientation of the two cavities. On the other hand, Abell 2052 depicts an intricate ongoing activity at the core site.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;On a conclusive note, the coupling between the radio galaxy and cavities still needs to be fully exploited. Jets and cavities heat the intracluster medium, thereby inhibiting cooling flow and affects accretion and galaxy growth. In fact, the nature of cavities, more precisely, its non-thermal nature opens a new world towards the study of magnetism in these characteristic sites. &lt;/p&gt;
</description>
        <pubDate>Fri, 07 Oct 2016 10:00:00 +0400</pubDate>
        <link>https://Harry45.github.io/blog/2016/10/A-Brief-Overview-Of-My-Undergraduate-Project</link>
        <guid isPermaLink="true">https://Harry45.github.io/blog/2016/10/A-Brief-Overview-Of-My-Undergraduate-Project</guid>
        
        
        <category>Research</category>
        
      </item>
    
  </channel>
</rss>
