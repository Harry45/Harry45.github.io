I"l%<p align="justify">Below is a collection of interesting papers. Some of them have been very useful and are relevant to my research.</p>

<p><b><font size="4">2021</font></b></p>
<ol type="1">

<li>Causal Inference in AI Education: A Primer (Journal of Causal Inference, <a href="https://ftp.cs.ucla.edu/pub/stat_ser/r509.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Explainable neural networks that simulate reasoning (Nature, <a href="https://www.nature.com/articles/s43588-021-00132-w.epdf?sharing_token=Miw3VgGy5tWlPYJdVJ_NiNRgN0jAjWel9jnR3ZoTv0Nu04nBX5HBWwG0EWSD7WoMPg01I2kvA-70AMspE1wn3KDNMu-NrG2-WshiYS8NiHktWggdumGumKEVHOMn9m7XjipTYbp0rLdLSefNF8BcQO2CECevKh-3SydcnQa0QN4%3D"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

</ol>

<p><b><font size="4">2020</font></b></p>
<ol type="1">

<li>Challenges in Deploying Machine Learning: a Survey of Case Studies (NeurIPS 2020, <a href="https://arxiv.org/pdf/2011.09926.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>The carbon impact of artifcial intelligence (Nature Machine Intelligence, <a href="https://www.nature.com/articles/s42256-020-0219-9.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence (Philosophy and Technology, Springer, <a href="https://link.springer.com/content/pdf/10.1007/s13347-020-00405-8.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>AI for social good: unlocking the opportunity for positive impact (Nature Communications, <a href="https://www.nature.com/articles/s41467-020-15871-z.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>A Survey of Deep Learning for Scientific Discovery (arXiv, <a href="https://arxiv.org/abs/2003.11755"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>The Case for Bayesian Deep Learning (arXiv, <a href="https://arxiv.org/abs/2001.10995"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Lagrangian Neural Networks (arXiv, <a href="https://arxiv.org/pdf/2003.04630.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Four Steps Towards Robust Artificial Intelligence (arXiv, <a href="https://arxiv.org/pdf/2002.06177.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
</ol>

<p><b><font size="4">2019</font></b></p>
<ol type="1">

<li>A deep learning framework for neuroscience (Nature Neuroscience, <a href="https://www.nature.com/articles/s41593-019-0520-2.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
<li>Posterior inference unchained with EL<sub>2</sub>O (arXiv, <a href="https://arxiv.org/abs/1901.04454"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Monte Carlo Gradient Estimation in Machine Learning (arXiv, <a href="https://arxiv.org/abs/1906.10652"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Tackling Climate Change with Machine Learning (arXiv, <a href="https://arxiv.org/abs/1906.05433"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Reconciling modern machine-learning practice and the classical bias–variance trade-off (PNAS, <a href="https://www.pnas.org/content/116/32/15849"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
</ol>

<p><b><font size="4">2018</font></b></p>
<ol type="1">
<li>Generalized massive optimal data compression (MNRAS, <a href="https://academic.oup.com/mnrasl/article/476/1/L60/4909822"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Massive optimal data compression and density estimation for scalable, likelihood-free inference in cosmology (MNRAS, <a href="https://academic.oup.com/mnras/article/477/3/2874/4956055"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>



<li>
Solving linear equations with messenger-field and conjugate gradient techniques (A&amp;A, <a href="https://www.aanda.org/articles/aa/full_html/2018/12/aa32987-18/aa32987-18.html"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
</ol>

<p><b><font size="4">2017</font></b></p>
<ol type="1">
<li>Wiener filter reloaded: fast signal reconstruction without preconditioning (MNRAS, <a href="https://academic.oup.com/mnras/article/468/2/1782/3059161"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Why Does Deep and Cheap Learning Work So Well? (Springer Nature, <a href="https://link.springer.com/article/10.1007%2Fs10955-017-1836-5"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

</ol>

<p><b><font size="4">2015</font></b></p>
<ol type="1">
<li>Probabilistic machine learning and artificial intelligence (Nature, <a href="https://www.nature.com/articles/nature14541"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
<li>Deep learning (Nature, <a href="https://www.nature.com/articles/nature14539"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
<li>Taking the Human Out of the Loop: A Review of Bayesian Optimization (IEEE, <a href="https://ieeexplore.ieee.org/document/7352306"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
</ol>

<p><b><font size="4">2013</font></b></p>
<ol type="1">

<li>Bayesian non-parametrics and the probabilistic approach to modelling (The Royal Society Publishing, <a href="https://doi.org/10.1098/rsta.2011.0553"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Efficient sampling of fast and slow cosmological parameters (Physical Review, <a href="https://journals.aps.org/prd/pdf/10.1103/PhysRevD.87.103529"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

</ol>

<p><b><font size="4">2012</font></b></p>
<ol type="1">

<li>Representation Learning: A Review and New Perspectives (arXiv, <a href="https://arxiv.org/pdf/1206.5538.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

</ol>

<p><b><font size="4">2011</font></b></p>
<ol type="1">

<li>Additive Gaussian Processes (arXiv, <a href="https://arxiv.org/abs/1112.4394"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Distributed Gaussian Processes (ICML, <a href="http://proceedings.mlr.press/v37/deisenroth15.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Intelligent design: on the emulation of cosmological simulations (IOP, <a href="https://iopscience.iop.org/article/10.1088/0004-637X/728/2/137"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo (JMLR, <a href="https://dl.acm.org/doi/10.5555/2627435.2638586"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

</ol>

<p><b><font size="4">2010</font></b></p>
<ol type="1">

<li>A Tutorial on Bayesian Optimization (arXiv, <a href="https://arxiv.org/abs/1012.2599"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

</ol>

<p><b><font size="4">Before 2010</font></b></p>
<ol type="1">


<li>Fast optimal CMB power spectrum estimation with Hamiltonian sampling (MNRAS, 2008, <a href="https://academic.oup.com/mnras/article/389/3/1284/1018688"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Bayes in the sky: Bayesian inference and model selection in cosmology (Contemporary Physics, 2008, <a href="https://www.tandfonline.com/doi/full/10.1080/00107510802066753"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Fast cosmological parameter estimation using neural networks (MNRAS, 2007, <a href="https://academic.oup.com/mnrasl/article/376/1/L11/957051"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Pico: Parameters for the Impatient Cosmologist (IOP, 2007, <a href="https://iopscience.iop.org/article/10.1086/508342/meta"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Random Features for Large-Scale Kernel Machines (NIPS, 2007, <a href="https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Efficient Cosmological Parameter Estimation with Hamiltonian Monte Carlo (APS, 2007, <a href="https://journals.aps.org/prd/abstract/10.1103/PhysRevD.75.083525"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Sparse Gaussian Processes using Pseudo-inputs (NIPS, 2005, <a href="https://papers.nips.cc/paper/2857-sparse-gaussian-processes-using-pseudo-inputs"><i style="font-size:12px" class="fa">&#xf08e;</i></a>)</li>

<li>A Bayesian Committee Machine (Neural Computation, 2000, <a href="https://www.mitpressjournals.org/doi/10.1162/089976600300014908"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Massive Lossless Data Compression and Multiple Parameter Estimation from Galaxy Spectra (MNRAS, 2000, <a href="https://academic.oup.com/mnras/article/317/4/965/1039456"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Karhunen-Loève Eigenvalue Problems in Cosmology: How Should We Tackle Large Data Sets? (IOP, 1997, <a href="https://iopscience.iop.org/article/10.1086/303939"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>No free lunch theorems for optimization (IEEE, 1997, <a href="https://ieeexplore.ieee.org/document/585893"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

</ol>

<p><b><font size="4">Classical Papers</font></b></p>
<ol type="1">

<li>A Bayesian approach to model inadequacy for polynomial regression (Biometrika, 1975, <a href="https://doi.org/10.1093/biomet/62.1.79"><i style="font-size:12px" class="fa">&#xf08e;</i></a>)</li>

</ol>

:ET