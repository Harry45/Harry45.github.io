---
layout: page
title: Papers
permalink: /papers/
---

Below is a collection of papers, which I have found interesting (useful and relevant for my research).



<p><b><font size="4">2020</font></b></p>
<ol type="1">
<li>AI for social good: unlocking the opportunity
for positive impact (Nature, <a href="https://www.nature.com/articles/s41467-020-15871-z.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
<li>A Survey of Deep Learning for Scientific Discovery (arXiv, <a href="https://arxiv.org/abs/2003.11755"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
<li>The Case for Bayesian Deep Learning (arXiv, <a href="https://arxiv.org/abs/2001.10995"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
<li>Lagrangian Neural Networks (arXiv, <a href="https://arxiv.org/pdf/2003.04630.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
</ol>

<p><b><font size="4">2019</font></b></p>
<ol type="1">
<li>Posterior inference unchained with EL<sub>2</sub>O (arXiv, <a href="https://arxiv.org/abs/1901.04454"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Monte Carlo Gradient Estimation in Machine Learning (arXiv, <a href="https://arxiv.org/abs/1906.10652"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Tackling Climate Change with Machine Learning (arXiv, <a href="https://arxiv.org/abs/1906.05433"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Reconciling modern machine-learning practice and the classical bias–variance trade-off (PNAS, <a href="https://www.pnas.org/content/116/32/15849"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
</ol>




<p><b><font size="4">2018</font></b></p>
<ol type="1">
<li>Generalized massive optimal data compression (MNRAS, <a href="https://academic.oup.com/mnrasl/article/476/1/L60/4909822"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Massive optimal data compression and density estimation for scalable, likelihood-free inference in cosmology (MNRAS, <a href="https://academic.oup.com/mnras/article/477/3/2874/4956055"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>



<li>
Solving linear equations with messenger-field and conjugate gradient techniques (A&A, <a href="https://www.aanda.org/articles/aa/full_html/2018/12/aa32987-18/aa32987-18.html"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
</ol>

<p><b><font size="4">2017</font></b></p>
<ol type="1">
<li>Wiener filter reloaded: fast signal reconstruction without preconditioning (MNRAS, <a href="https://academic.oup.com/mnras/article/468/2/1782/3059161"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Why Does Deep and Cheap Learning Work So Well? (Springer Nature, <a href="https://link.springer.com/article/10.1007%2Fs10955-017-1836-5"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

</ol>


<p><b><font size="4">2015</font></b></p>
<ol type="1">
<li>Probabilistic machine learning and artificial intelligence (Nature, <a href="https://www.nature.com/articles/nature14541"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
<li>Deep learning (Nature, <a href="https://www.nature.com/articles/nature14539"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
<li>Taking the Human Out of the Loop: A Review of Bayesian Optimization (IEEE, <a href="https://ieeexplore.ieee.org/document/7352306"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
</ol>


<p><b><font size="4">2011</font></b></p>
<ol type="1">

<li>Additive Gaussian Processes (arXiv, <a href="https://arxiv.org/abs/1112.4394"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Distributed Gaussian Processes (ICML, <a href="http://proceedings.mlr.press/v37/deisenroth15.pdf"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Intelligent design: on the emulation of cosmological simulations (IOP, <a href="https://iopscience.iop.org/article/10.1088/0004-637X/728/2/137"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo (JMLR, <a href="https://dl.acm.org/doi/10.5555/2627435.2638586"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

</ol>


<p><b><font size="4">2010</font></b></p>
<ol type="1">

<li>A Tutorial on Bayesian Optimization (arXiv, <a href="https://arxiv.org/abs/1012.2599"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

</ol>


<p><b><font size="4">Before 2010</font></b></p>
<ol type="1">


<li>Fast optimal CMB power spectrum estimation with Hamiltonian sampling (MNRAS, 2008, <a href="https://academic.oup.com/mnras/article/389/3/1284/1018688"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Bayes in the sky: Bayesian inference and model selection in cosmology (Contemporary Physics, 2008, <a href="https://www.tandfonline.com/doi/full/10.1080/00107510802066753"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Fast cosmological parameter estimation using neural networks (MNRAS, 2007, <a href="https://academic.oup.com/mnrasl/article/376/1/L11/957051"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Pico: Parameters for the Impatient Cosmologist (IOP, 2007, <a href="https://iopscience.iop.org/article/10.1086/508342/meta"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Random Features for Large-Scale Kernel Machines (NIPS, 2007, <a href="https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Efficient Cosmological Parameter Estimation with Hamiltonian Monte Carlo (APS, 2007, <a href="https://journals.aps.org/prd/abstract/10.1103/PhysRevD.75.083525"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Sparse Gaussian Processes using Pseudo-inputs (NIPS, 2005, <a href="https://papers.nips.cc/paper/2857-sparse-gaussian-processes-using-pseudo-inputs"><i style="font-size:12px" class="fa">&#xf08e;</i></a>)</li>

<li>A Bayesian Committee Machine (Neural Computation, 2000, <a href="https://www.mitpressjournals.org/doi/10.1162/089976600300014908"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Massive Lossless Data Compression and Multiple Parameter Estimation from Galaxy Spectra (MNRAS, 2000, <a href="https://academic.oup.com/mnras/article/317/4/965/1039456"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>

<li>Karhunen-Loève Eigenvalue Problems in Cosmology: How Should We Tackle Large Data Sets? (IOP, 1997, <a href="https://iopscience.iop.org/article/10.1086/303939"><i style="font-size:12px" class="fa">&#xf08e;</i></a>) </li>
</ol>


<p><b><font size="4">Classical Papers</font></b></p>
<ol type="1">

<li>A Bayesian approach to model inadequacy for polynomial regression (Biometrika, 1975, <a href="https://doi.org/10.1093/biomet/62.1.79"><i style="font-size:12px" class="fa">&#xf08e;</i></a>)</li>

</ol>


